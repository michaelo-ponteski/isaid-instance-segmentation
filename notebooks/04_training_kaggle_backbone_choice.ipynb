{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a147ff",
   "metadata": {},
   "source": [
    "# Training with Pretrained ResNet Backbones (W&B Integration)\n",
    "\n",
    "This notebook demonstrates iSAID instance segmentation training using **pretrained ResNet-50/ResNet-101** backbones with FPN from torchvision, instead of our custom EfficientNet + CBAM backbone.\n",
    "\n",
    "**Features:**\n",
    "\n",
    "- Uses torchvision's pretrained ResNet-50-FPN or ResNet-101-FPN Mask R-CNN\n",
    "- Automatic logging of training/validation losses and metrics\n",
    "- Learning rate scheduling (OneCycleLR or ReduceLROnPlateau)\n",
    "- Validation predictions visualization\n",
    "- Model checkpointing as W&B artifacts\n",
    "- mAP, mean IoU, and overfitting gap metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4d5f9",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2aae81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'isaid-instance-segmentation'...\n",
      "remote: Enumerating objects: 496, done.\u001b[K\n",
      "remote: Counting objects: 100% (218/218), done.\u001b[K\n",
      "remote: Compressing objects: 100% (148/148), done.\u001b[K\n",
      "remote: Total 496 (delta 130), reused 139 (delta 70), pack-reused 278 (from 1)\u001b[K\n",
      "Receiving objects: 100% (496/496), 5.50 MiB | 21.24 MiB/s, done.\n",
      "Resolving deltas: 100% (258/258), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/michaelo-ponteski/isaid-instance-segmentation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ff5aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/isaid-instance-segmentation\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "%cd isaid-instance-segmentation/\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62870855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "Available memory: 42.5 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Set memory optimization for CUDA\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dd11d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b6de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb version: 0.24.0\n"
     ]
    }
   ],
   "source": [
    "# Install wandb if not available\n",
    "try:\n",
    "    import wandb\n",
    "    print(f\"wandb version: {wandb.__version__}\") # Must be newest\n",
    "except ImportError:\n",
    "    print(\"Installing wandb...\")\n",
    "    !pip install --upgrade wandb\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232354fa",
   "metadata": {},
   "source": [
    "### Kaggle wandb API setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86fd40ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichaelo-ponteski\u001b[0m (\u001b[33mmarek-olnk-put-pozna-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(\n",
    "    key=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9c3aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully!\n",
      "\n",
      "iSAID Class Labels:\n",
      "  0: background\n",
      "  1: ship\n",
      "  2: storage_tank\n",
      "  3: baseball_diamond\n",
      "  4: tennis_court\n",
      "  5: basketball_court\n",
      "  6: ground_track_field\n",
      "  7: bridge\n",
      "  8: large_vehicle\n",
      "  9: small_vehicle\n",
      "  10: helicopter\n",
      "  11: swimming_pool\n",
      "  12: roundabout\n",
      "  13: soccer_ball_field\n",
      "  14: plane\n",
      "  15: harbor\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import datasets.isaid_dataset\n",
    "import training.transforms\n",
    "import training.trainer\n",
    "\n",
    "importlib.reload(datasets.isaid_dataset)\n",
    "importlib.reload(training.transforms)\n",
    "importlib.reload(training.trainer)\n",
    "\n",
    "from datasets.isaid_dataset import iSAIDDataset\n",
    "from training.transforms import get_transforms\n",
    "from training.trainer import Trainer, create_datasets\n",
    "from training.wandb_logger import ISAID_CLASS_LABELS\n",
    "\n",
    "# Import torchvision's pretrained Mask R-CNN models\n",
    "from torchvision.models.detection import (\n",
    "    maskrcnn_resnet50_fpn,\n",
    "    maskrcnn_resnet50_fpn_v2,\n",
    "    MaskRCNN_ResNet50_FPN_Weights,\n",
    "    MaskRCNN_ResNet50_FPN_V2_Weights,\n",
    ")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "print(\"All modules imported successfully!\")\n",
    "print(f\"\\niSAID Class Labels:\")\n",
    "for idx, name in ISAID_CLASS_LABELS.items():\n",
    "    print(f\"  {idx}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a3fdb",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c38ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/michaeloponteski/isaid-patches?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41.1G/41.1G [32:13<00:00, 22.8MB/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /root/.cache/kagglehub/datasets/michaeloponteski/isaid-patches/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"michaeloponteski/isaid-patches\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dfcf77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = path + \"/iSAID_patches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c3db230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "  data_root: /root/.cache/kagglehub/datasets/michaeloponteski/isaid-patches/versions/1/iSAID_patches\n",
      "  num_classes: 16\n",
      "  image_size: 800\n",
      "  batch_size: 8\n",
      "  val_batch_size: 8\n",
      "  num_epochs: 20\n",
      "  learning_rate: 0.0003\n",
      "  weight_decay: 0.001\n",
      "  momentum: 0.9\n",
      "  backbone: resnet50\n",
      "  pretrained_backbone: True\n",
      "  pretrained_coco: True\n",
      "  anchor_sizes: ((8, 16), (16, 32), (32, 64), (64, 128), (128, 256))\n",
      "  aspect_ratios: ((0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0))\n",
      "  wandb_project: isaid-resnet50-segmentation\n",
      "  wandb_entity: marek-olnk-put-pozna-\n",
      "  wandb_log_freq: 20\n",
      "  wandb_num_val_images: 4\n",
      "  wandb_conf_threshold: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Choose backbone: \"resnet50\" or \"resnet101\"\n",
    "# Note: torchvision provides resnet50_fpn pretrained, for resnet101 we build it manually\n",
    "BACKBONE_CHOICE = \"resnet50\"  # Options: \"resnet50\", \"resnet50_v2\", \"resnet101\"\n",
    "\n",
    "# All hyperparameters in one place - this will be logged to W&B\n",
    "HYPERPARAMETERS = {\n",
    "    # Dataset\n",
    "    \"data_root\": root_dir,\n",
    "    \"num_classes\": 16,\n",
    "    \"image_size\": 800,\n",
    "    # Training\n",
    "    \"batch_size\": 8,\n",
    "    \"val_batch_size\": 8,\n",
    "    \"num_epochs\": 20,\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "    # Model Architecture\n",
    "    \"backbone\": BACKBONE_CHOICE,\n",
    "    \"pretrained_backbone\": True,\n",
    "    \"pretrained_coco\": True,  # Use COCO pretrained weights\n",
    "    # RPN Anchors (optimized for iSAID)\n",
    "    \"anchor_sizes\": ((8, 16), (16, 32), (32, 64), (64, 128), (128, 256)),\n",
    "    \"aspect_ratios\": ((0.5, 1.0, 2.0),) * 5,\n",
    "    # W&B Logging\n",
    "    \"wandb_project\": \"isaid-resnet50-segmentation\",\n",
    "    \"wandb_entity\": \"marek-olnk-put-pozna-\",\n",
    "    \"wandb_log_freq\": 20,  # Log every N batches\n",
    "    \"wandb_num_val_images\": 4,  # Number of images for validation visualization\n",
    "    \"wandb_conf_threshold\": 0.5,  # Confidence threshold for predictions\n",
    "}\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "for k, v in HYPERPARAMETERS.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab2e51c",
   "metadata": {},
   "source": [
    "## 3. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d64cb9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "============================================================\n",
      "DATASET STATISTICS: TRAIN\n",
      "============================================================\n",
      "\n",
      "Image Counts:\n",
      "   Original images:        28029\n",
      "   Final images:           26431\n",
      "\n",
      "Rejected Images (1598 total):\n",
      "   - Too many boxes (>400): 230\n",
      "   - Empty image excess:       1368\n",
      "\n",
      "Box Distribution (final dataset):\n",
      "   Empty images (0 boxes):  7929 (30.0%)\n",
      "   Non-empty images:        18502\n",
      "\n",
      "Box Count Statistics:\n",
      "   Min:    0\n",
      "   Max:    400\n",
      "   Mean:   21.2\n",
      "   Median: 3.0\n",
      "   Std:    47.2\n",
      "\n",
      "   Percentiles:\n",
      "     25th: 0\n",
      "     50th: 3\n",
      "     75th: 18\n",
      "     90th: 58\n",
      "     95th: 106\n",
      "     99th: 262\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "DATASET STATISTICS: VAL\n",
      "============================================================\n",
      "\n",
      "Image Counts:\n",
      "   Original images:        9512\n",
      "   Final images:           8551\n",
      "\n",
      "Rejected Images (961 total):\n",
      "   - Too many boxes (>400): 62\n",
      "   - Empty image excess:       899\n",
      "\n",
      "Box Distribution (final dataset):\n",
      "   Empty images (0 boxes):  2565 (30.0%)\n",
      "   Non-empty images:        5986\n",
      "\n",
      "Box Count Statistics:\n",
      "   Min:    0\n",
      "   Max:    400\n",
      "   Mean:   22.0\n",
      "   Median: 3.0\n",
      "   Std:    48.0\n",
      "\n",
      "   Percentiles:\n",
      "     25th: 0\n",
      "     50th: 3\n",
      "     75th: 20\n",
      "     90th: 61\n",
      "     95th: 110\n",
      "     99th: 264\n",
      "============================================================\n",
      "\n",
      "Using full dataset: 26431 train, 8551 val samples\n",
      "Train samples: 26431\n",
      "Val samples: 8551\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset, val_dataset = create_datasets(\n",
    "    data_root=HYPERPARAMETERS[\"data_root\"],\n",
    "    image_size=HYPERPARAMETERS[\"image_size\"],\n",
    "    subset_fraction=1.0,  # Use full dataset\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5672c8",
   "metadata": {},
   "source": [
    "## 4. Create Model with Pretrained ResNet Backbone\n",
    "\n",
    "We use torchvision's pretrained Mask R-CNN models and modify the prediction heads for our number of classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d8787fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:00<00:00, 247MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone: resnet50\n",
      "Total parameters: 43,997,743\n",
      "Trainable parameters: 43,775,343\n",
      "Model size: 176.0 MB (FP32)\n"
     ]
    }
   ],
   "source": [
    "def create_maskrcnn_resnet(num_classes, backbone_type=\"resnet50\", pretrained_coco=True):\n",
    "    \"\"\"\n",
    "    Create Mask R-CNN with pretrained ResNet backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of classes (including background)\n",
    "        backbone_type: \"resnet50\", \"resnet50_v2\", or \"resnet101\"\n",
    "        pretrained_coco: Whether to use COCO pretrained weights\n",
    "\n",
    "    Returns:\n",
    "        Mask R-CNN model\n",
    "    \"\"\"\n",
    "    if backbone_type == \"resnet50\":\n",
    "        # ResNet-50 FPN (original)\n",
    "        if pretrained_coco:\n",
    "            weights = MaskRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "            model = maskrcnn_resnet50_fpn(weights=weights)\n",
    "        else:\n",
    "            model = maskrcnn_resnet50_fpn(\n",
    "                weights=None, weights_backbone=\"IMAGENET1K_V1\"\n",
    "            )\n",
    "\n",
    "    elif backbone_type == \"resnet50_v2\":\n",
    "        # ResNet-50 FPN V2 (improved, better performance)\n",
    "        if pretrained_coco:\n",
    "            weights = MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "            model = maskrcnn_resnet50_fpn_v2(weights=weights)\n",
    "        else:\n",
    "            model = maskrcnn_resnet50_fpn_v2(\n",
    "                weights=None, weights_backbone=\"IMAGENET1K_V1\"\n",
    "            )\n",
    "\n",
    "    elif backbone_type == \"resnet101\":\n",
    "        # ResNet-101 FPN - build manually using backbone_resnet with resnet101\n",
    "        from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "        from torchvision.models.detection import MaskRCNN\n",
    "        from torchvision.models import ResNet101_Weights\n",
    "\n",
    "        # Create ResNet-101 FPN backbone\n",
    "        backbone = resnet_fpn_backbone(\n",
    "            backbone_name=\"resnet101\",\n",
    "            weights=ResNet101_Weights.IMAGENET1K_V1 if pretrained_coco else None,\n",
    "            trainable_layers=5,  # Train all layers\n",
    "        )\n",
    "\n",
    "        # Create Mask R-CNN with ResNet-101 backbone\n",
    "        model = MaskRCNN(\n",
    "            backbone,\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "        return model  # Already has correct num_classes\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unknown backbone type: {backbone_type}. Use 'resnet50', 'resnet50_v2', or 'resnet101'\"\n",
    "        )\n",
    "\n",
    "    # For resnet50/resnet50_v2: Replace the pre-trained head with a new one for our num_classes\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # Replace the box predictor\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Replace the mask predictor\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask, hidden_layer, num_classes\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = create_maskrcnn_resnet(\n",
    "    num_classes=HYPERPARAMETERS[\"num_classes\"],\n",
    "    backbone_type=HYPERPARAMETERS[\"backbone\"],\n",
    "    pretrained_coco=HYPERPARAMETERS[\"pretrained_coco\"],\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Backbone: {HYPERPARAMETERS['backbone']}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1e6:.1f} MB (FP32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c65ef8",
   "metadata": {},
   "source": [
    "## 5. Create Trainer with W&B Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7af2dbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided datasets: 26431 train, 8551 val samples\n",
      "Using provided model\n",
      "Optimizer parameter groups:\n",
      "  Base params: 64 tensors, lr=3.00e-04\n",
      "  RoI params:  20 tensors, lr=7.50e-05 (alpha=0.25)\n",
      "Device: cuda\n",
      "AMP enabled: True\n",
      "Train samples: 26431\n",
      "Val samples: 8551\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/isaid-instance-segmentation/wandb/run-20260127_000205-vhozj3u4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marek-olnk-put-pozna-/isaid-resnet50-segmentation/runs/vhozj3u4' target=\"_blank\">ethereal-wildflower-1</a></strong> to <a href='https://wandb.ai/marek-olnk-put-pozna-/isaid-resnet50-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marek-olnk-put-pozna-/isaid-resnet50-segmentation' target=\"_blank\">https://wandb.ai/marek-olnk-put-pozna-/isaid-resnet50-segmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marek-olnk-put-pozna-/isaid-resnet50-segmentation/runs/vhozj3u4' target=\"_blank\">https://wandb.ai/marek-olnk-put-pozna-/isaid-resnet50-segmentation/runs/vhozj3u4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run initialized: ethereal-wildflower-1\n",
      "View at: https://wandb.ai/marek-olnk-put-pozna-/isaid-resnet50-segmentation/runs/vhozj3u4\n",
      "Selected 4 validation images for visualization\n",
      "W&B logging enabled: https://wandb.ai/marek-olnk-put-pozna-/isaid-resnet50-segmentation/runs/vhozj3u4\n",
      "\n",
      "W&B Run: ethereal-wildflower-1\n",
      "URL: https://wandb.ai/marek-olnk-put-pozna-/isaid-resnet50-segmentation/runs/vhozj3u4\n"
     ]
    }
   ],
   "source": [
    "# Create trainer with W&B integration\n",
    "# The trainer handles all logging automatically!\n",
    "trainer = Trainer(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    model=model,\n",
    "    batch_size=HYPERPARAMETERS[\"batch_size\"],\n",
    "    val_batch_size=HYPERPARAMETERS[\"val_batch_size\"],\n",
    "    lr=HYPERPARAMETERS[\"learning_rate\"],\n",
    "    device=device,\n",
    "    use_amp=True,\n",
    "    num_workers=4,\n",
    "    # W&B configuration\n",
    "    wandb_project=HYPERPARAMETERS[\"wandb_project\"],\n",
    "    wandb_entity=HYPERPARAMETERS[\"wandb_entity\"],\n",
    "    wandb_tags=[\n",
    "        \"maskrcnn\",\n",
    "        HYPERPARAMETERS[\"backbone\"],\n",
    "        \"pretrained\",\n",
    "        \"trainer-integrated\",\n",
    "    ],\n",
    "    wandb_notes=f\"Training with {HYPERPARAMETERS['backbone']} backbone (COCO pretrained) + FPN\",\n",
    "    wandb_log_freq=HYPERPARAMETERS[\"wandb_log_freq\"],\n",
    "    wandb_num_val_images=HYPERPARAMETERS[\"wandb_num_val_images\"],\n",
    "    wandb_conf_threshold=HYPERPARAMETERS[\"wandb_conf_threshold\"],\n",
    "    hyperparameters=HYPERPARAMETERS,\n",
    ")\n",
    "\n",
    "print(f\"\\nW&B Run: {trainer.wandb_logger.run.name}\")\n",
    "print(f\"URL: {trainer.wandb_logger.run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a053be",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "The `Trainer.fit()` method handles everything:\n",
    "\n",
    "- Training loop with gradient clipping and AMP\n",
    "- Validation loss computation\n",
    "- mAP and mean IoU metrics\n",
    "- W&B logging (losses, gradients, predictions, checkpoints)\n",
    "- Learning rate scheduling\n",
    "- Best model saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97299b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ReduceLROnPlateau scheduler (steps on validation mAP)\n",
      "\n",
      "============================================================\n",
      "Epoch 1/20 | LR: 3.00e-04\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4f00bc3d01441b8e860d3d76a7b631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 1:   0%|          | 0/3303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cf0c1c8c994679bd692a3cde713af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/1069 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mAP metrics...\n",
      "\n",
      "Epoch 1 Results (Time: 1380.3s):\n",
      "  Losses:\n",
      "    Train: 0.9477\n",
      "    Val:   0.7755\n",
      "  Performance Metrics:\n",
      "    Train mAP@0.5: 0.4175\n",
      "    Val mAP@0.5:   0.4154 (primary metric)\n",
      "    Val Mean IoU:  0.4628\n",
      "  Training Dynamics:\n",
      "    Gradient Norm: nan\n",
      "    Loss Variance: 0.160097\n",
      "    mAP Gap (train-val): +0.0022\n",
      "  Detailed Train Losses:\n",
      "    loss_classifier: 0.2122\n",
      "    loss_box_reg: 0.1920\n",
      "    loss_mask: 0.3018\n",
      "    loss_objectness: 0.1352\n",
      "    loss_rpn_box_reg: 0.1065\n",
      "-> New best model saved (by loss)\n",
      "Model checkpoint logged as artifact: isaid-model-best-val-loss\n",
      "-> New best val mAP@0.5: 0.4154\n",
      "Model checkpoint logged as artifact: isaid-model-best-train-map\n",
      "-> New best train mAP@0.5: 0.4175\n",
      "\n",
      "============================================================\n",
      "Epoch 2/20 | LR: 3.00e-04\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983595a628934308bacc6cdb8140f401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 2:   0%|          | 0/3303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run training!\n",
    "# All W&B logging happens automatically inside trainer.fit()\n",
    "history = trainer.fit(\n",
    "    epochs=HYPERPARAMETERS[\"num_epochs\"],\n",
    "    save_dir=\"checkpoints\",\n",
    "    compute_metrics_every=1,  # Compute mAP every epoch\n",
    "    max_map_samples=200,  # Limit samples for faster mAP computation\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be24de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artifact for the final trained model\n",
    "artifact = wandb.Artifact(\n",
    "    name=f\"isaid-maskrcnn-{HYPERPARAMETERS['backbone']}-final\",\n",
    "    type=\"model\",\n",
    "    description=f\"Final trained Mask R-CNN ({HYPERPARAMETERS['backbone']}) after {HYPERPARAMETERS['num_epochs']} epochs\",\n",
    "    metadata={\n",
    "        \"backbone\": HYPERPARAMETERS[\"backbone\"],\n",
    "        \"num_classes\": HYPERPARAMETERS[\"num_classes\"],\n",
    "        \"pretrained_coco\": HYPERPARAMETERS[\"pretrained_coco\"],\n",
    "        \"final_train_loss\": history[\"train/loss\"][-1],\n",
    "        \"final_val_loss\": history[\"val/loss\"][-1],\n",
    "        \"final_val_mAP\": history[\"val/mAP@0.5\"][-1],\n",
    "        \"best_val_mAP\": max(history[\"val/mAP@0.5\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add model checkpoint files\n",
    "artifact.add_file(\"checkpoints/best.pth\", name=\"best_model.pth\")\n",
    "artifact.add_file(\"checkpoints/best_map.pth\", name=\"best_map_model.pth\")\n",
    "artifact.add_file(\"checkpoints/last.pth\", name=\"last_model.pth\")\n",
    "\n",
    "# Log the artifact\n",
    "trainer.wandb_logger.run.log_artifact(artifact)\n",
    "\n",
    "print(f\"Model artifacts saved to W&B!\")\n",
    "print(f\"  - best_model.pth (lowest val loss)\")\n",
    "print(f\"  - best_map_model.pth (highest val mAP)\")\n",
    "print(f\"  - last_model.pth (final epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e28c9b",
   "metadata": {},
   "source": [
    "## 7. Visualize Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cff6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history[\"train/loss\"], label=\"Train Loss\")\n",
    "ax.plot(history[\"val/loss\"], label=\"Val Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training & Validation Loss\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# mAP curves\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history[\"train/mAP@0.5\"], label=\"Train mAP@0.5\")\n",
    "ax.plot(history[\"val/mAP@0.5\"], label=\"Val mAP@0.5\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"mAP@0.5\")\n",
    "ax.set_title(\"mAP Performance\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history[\"train/lr\"])\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Learning Rate\")\n",
    "ax.set_title(\"Learning Rate Schedule\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history[\"train/grad_norm\"])\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Gradient Norm\")\n",
    "ax.set_title(\"Training Gradient Norm\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acc1bb",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on validation set\n",
    "trainer.visualize_predictions(\n",
    "    num_samples=5,\n",
    "    score_threshold=0.5,\n",
    "    mask_alpha=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8d21f",
   "metadata": {},
   "source": [
    "## 9. Finish W&B Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the W&B run\n",
    "trainer.finish()\n",
    "\n",
    "print(f\"\\nW&B run completed!\")\n",
    "print(f\"View results at: {trainer.wandb_logger.run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84608e2b",
   "metadata": {},
   "source": [
    "## 10. Load Model from W&B Artifact (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load best model from W&B artifacts\n",
    "# Uncomment to use\n",
    "\n",
    "# import wandb\n",
    "# api = wandb.Api()\n",
    "# artifact = api.artifact('YOUR_ENTITY/isaid-resnet-segmentation/isaid-model:best')\n",
    "# artifact_dir = artifact.download()\n",
    "#\n",
    "# # Recreate model with same architecture\n",
    "# model = create_maskrcnn_resnet(num_classes=16, backbone_type=\"resnet50\")\n",
    "# model.load_state_dict(torch.load(f\"{artifact_dir}/best_model.pth\"))\n",
    "# model.eval()\n",
    "# print(\"Model loaded from W&B artifact!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

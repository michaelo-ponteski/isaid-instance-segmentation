{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432acec7",
   "metadata": {},
   "source": [
    "# Mask R-CNN Inference Visualization Pipeline\n",
    "\n",
    "This notebook provides comprehensive visualization of **every stage** of our Custom Mask R-CNN during inference:\n",
    "\n",
    "## Pipeline Stages Visualized:\n",
    "1. **Input Image** - Original image preprocessing\n",
    "2. **EfficientNet Backbone** - Feature extraction at multiple scales (C2-C5)\n",
    "3. **CBAM Attention** - Channel and Spatial attention at each backbone stage\n",
    "4. **Feature Pyramid Network (FPN)** - Multi-scale feature fusion (P2-P5)\n",
    "5. **FPN Attention** - CBAM attention on FPN outputs\n",
    "6. **Region Proposal Network (RPN)** - Anchor-based proposals\n",
    "7. **RoI Align** - Feature extraction with bilinear sampling\n",
    "8. **Box Head** - FC layers for classification and regression\n",
    "9. **Mask Head** - Convolutional layers for segmentation\n",
    "10. **Grad-CAM Heatmaps** - Class activation maps showing \"where the model looks\"\n",
    "11. **Final Predictions** - Boxes, classes, and masks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5fb565",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent if 'notebooks' in os.getcwd() else Path(os.getcwd())\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3cb1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Enable interactive matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2104fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization pipeline\n",
    "from visualization.gradcam_pipeline import (\n",
    "    MaskRCNNVisualizationPipeline,\n",
    "    GradCAMConfig,\n",
    "    load_pipeline,\n",
    "    denormalize_image,\n",
    "    overlay_heatmap,\n",
    "    visualize_feature_maps,\n",
    "    visualize_fpn_features,\n",
    "    visualize_roi_align_grid,\n",
    "    visualize_box_head_features,\n",
    "    visualize_mask_head_stages,\n",
    "    visualize_final_predictions,\n",
    "    ISAID_CLASS_LABELS,\n",
    "    ISAID_COLORS,\n",
    ")\n",
    "\n",
    "from models.maskrcnn_model import get_custom_maskrcnn\n",
    "\n",
    "print(\"Visualization pipeline imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michaelo-ponteski/isaid-instance-segmentation.git\n",
    "%cd /kaggle/working/isaid-instance-segmentation\n",
    "!git pull\n",
    "!git switch gradcam\n",
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e686887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "my_secret = user_secrets.get_secret(\"wandb_key\")\n",
    "wandb.login(key=my_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74650e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init()\n",
    "artifact = run.use_artifact('marek-olnk-put-pozna-/isaid-custom-segmentation/isaid-model:v21', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242849a",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your model checkpoint path and image path here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION \n",
    "# =============================================================================\n",
    "\n",
    "# Path to your trained model checkpoint\n",
    "CHECKPOINT_PATH = artifact_dir\n",
    "\n",
    "# Path to an image for inference (can be from validation set)\n",
    "IMAGE_PATH = \"data/val/images/sample.png\"  # <-- CHANGE THIS\n",
    "\n",
    "# Or use a sample from the dataset\n",
    "USE_DATASET_SAMPLE = True  # Set to True to load from dataset\n",
    "DATASET_ROOT = \"data\"  # Path to iSAID dataset\n",
    "SAMPLE_INDEX = 0  # Which sample to visualize\n",
    "\n",
    "# Model configuration (must match training)\n",
    "NUM_CLASSES = 16  # iSAID has 15 classes + background\n",
    "\n",
    "# Visualization settings\n",
    "CONF_THRESHOLD = 0.5  # Confidence threshold for predictions\n",
    "OUTPUT_DIR = \"./gradcam_outputs\"  # Where to save visualizations\n",
    "\n",
    "print(\"Configuration set!\")\n",
    "print(f\"  Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa08a8f",
   "metadata": {},
   "source": [
    "## 3. Load Model and Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e107ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization configuration\n",
    "config = GradCAMConfig(\n",
    "    device=device,\n",
    "    conf_threshold=CONF_THRESHOLD,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    colormap='jet',\n",
    "    alpha_overlay=0.5,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "print(\"Creating model...\")\n",
    "model = get_custom_maskrcnn(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    pretrained_backbone=False,  # We'll load trained weights\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = MaskRCNNVisualizationPipeline(model, config)\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  - Backbone: EfficientNet-B0 with CBAM attention\")\n",
    "print(f\"  - FPN: Custom with attention modules\")\n",
    "print(f\"  - Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a1373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = PROJECT_ROOT / CHECKPOINT_PATH\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    pipeline.load_model_weights(str(checkpoint_path))\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at: {checkpoint_path}\")\n",
    "    print(\"Using randomly initialized weights for demonstration.\")\n",
    "    print(\"\\nTo use trained weights, update CHECKPOINT_PATH in the configuration cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2e0de",
   "metadata": {},
   "source": [
    "## 4. Load Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2128f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "if USE_DATASET_SAMPLE:\n",
    "    # Load from dataset\n",
    "    try:\n",
    "        from datasets.isaid_dataset import get_isaid_dataset\n",
    "        from training.transforms import get_transform\n",
    "        \n",
    "        val_dataset = get_isaid_dataset(\n",
    "            root=str(PROJECT_ROOT / DATASET_ROOT),\n",
    "            split='val',\n",
    "            transforms=get_transform(train=False),\n",
    "        )\n",
    "        \n",
    "        # Get sample\n",
    "        image_tensor, target = val_dataset[SAMPLE_INDEX]\n",
    "        \n",
    "        # Denormalize for visualization\n",
    "        image_np = denormalize_image(image_tensor)\n",
    "        \n",
    "        print(f\"Loaded sample {SAMPLE_INDEX} from validation set\")\n",
    "        print(f\"  Image shape: {image_tensor.shape}\")\n",
    "        print(f\"  Number of GT objects: {len(target['boxes'])}\")\n",
    "        \n",
    "        # Show ground truth classes\n",
    "        gt_classes = [ISAID_CLASS_LABELS[l.item()] for l in target['labels']]\n",
    "        print(f\"  GT classes: {gt_classes}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load from dataset: {e}\")\n",
    "        print(\"Falling back to image path...\")\n",
    "        USE_DATASET_SAMPLE = False\n",
    "\n",
    "if not USE_DATASET_SAMPLE:\n",
    "    # Load from file\n",
    "    image_path = PROJECT_ROOT / IMAGE_PATH\n",
    "    \n",
    "    if image_path.exists():\n",
    "        image_np = np.array(Image.open(image_path).convert('RGB'))\n",
    "        print(f\"Loaded image from {image_path}\")\n",
    "        print(f\"  Image shape: {image_np.shape}\")\n",
    "    else:\n",
    "        # Create a dummy image for demonstration\n",
    "        print(f\"Image not found at {image_path}\")\n",
    "        print(\"Creating a dummy test image...\")\n",
    "        image_np = np.random.randint(0, 255, (800, 800, 3), dtype=np.uint8)\n",
    "\n",
    "# Display the input image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_np)\n",
    "plt.title('Input Image', fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586b519",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Stage-by-Stage Visualization\n",
    "\n",
    "Now we'll visualize each stage of the inference pipeline step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73378f3",
   "metadata": {},
   "source": [
    "## Stage 1: Preprocessing & Feature Extraction Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83470d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image and run inference with feature extraction\n",
    "print(\"Running inference with feature extraction hooks...\")\n",
    "\n",
    "# Preprocess\n",
    "image_tensor, image_display = pipeline.preprocess_image(image_np)\n",
    "print(f\"Preprocessed tensor shape: {image_tensor.shape}\")\n",
    "\n",
    "# Run inference with hooks\n",
    "results = pipeline.run_inference_with_gradients()\n",
    "\n",
    "predictions = results['predictions']\n",
    "fpn_features = results['fpn_features']\n",
    "extracted = results['extracted_features']\n",
    "\n",
    "print(f\"\\n Inference complete!\")\n",
    "print(f\"   Detections found: {len(predictions['boxes'])}\")\n",
    "print(f\"   Extracted features: {len(extracted)} tensors\")\n",
    "print(f\"   FPN levels: {list(fpn_features.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee2aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all extracted features\n",
    "print(\"\\n Extracted Features Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, tensor in sorted(extracted.items()):\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        shape = list(tensor.shape)\n",
    "        print(f\"  {name:40s} -> {shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d144db",
   "metadata": {},
   "source": [
    "## Stage 2: EfficientNet Backbone Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize backbone stages\n",
    "backbone_stages = {\n",
    "    'backbone_stage_3': 'C2 (After MBConv1)',\n",
    "    'backbone_stage_4': 'C3 (After MBConv2)', \n",
    "    'backbone_stage_5': 'C4 (After MBConv3)',\n",
    "    'backbone_stage_7': 'C5 (Final Features)',\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "ax_idx = 0\n",
    "for stage_key, stage_name in backbone_stages.items():\n",
    "    if stage_key in extracted:\n",
    "        feat = extracted[stage_key]\n",
    "        if feat.dim() == 4:\n",
    "            feat = feat[0]\n",
    "        \n",
    "        # Mean activation\n",
    "        feat_mean = feat.mean(dim=0).cpu().numpy()\n",
    "        feat_mean = (feat_mean - feat_mean.min()) / (feat_mean.max() - feat_mean.min() + 1e-8)\n",
    "        \n",
    "        axes[ax_idx].imshow(feat_mean, cmap='viridis')\n",
    "        axes[ax_idx].set_title(f'{stage_name}\\nMean ({feat.shape[0]} channels)', fontsize=10)\n",
    "        axes[ax_idx].axis('off')\n",
    "        ax_idx += 1\n",
    "        \n",
    "        # Max activation\n",
    "        feat_max = feat.max(dim=0)[0].cpu().numpy()\n",
    "        feat_max = (feat_max - feat_max.min()) / (feat_max.max() - feat_max.min() + 1e-8)\n",
    "        \n",
    "        axes[ax_idx].imshow(feat_max, cmap='hot')\n",
    "        axes[ax_idx].set_title(f'{stage_name}\\nMax Activation', fontsize=10)\n",
    "        axes[ax_idx].axis('off')\n",
    "        ax_idx += 1\n",
    "\n",
    "# Hide unused axes\n",
    "for i in range(ax_idx, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "fig.suptitle('Stage 2: EfficientNet Backbone Feature Maps', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5195dd",
   "metadata": {},
   "source": [
    "## Stage 3: CBAM Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb3ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CBAM attention outputs at each backbone stage\n",
    "cbam_stages = [\n",
    "    ('cbam_attention_c2', 'CBAM @ C2 (40 channels)'),\n",
    "    ('cbam_attention_c3', 'CBAM @ C3 (80 channels)'),\n",
    "    ('cbam_attention_c4', 'CBAM @ C4 (112 channels)'),\n",
    "    ('cbam_attention_c5', 'CBAM @ C5 (320 channels)'),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "for idx, (stage_key, stage_name) in enumerate(cbam_stages):\n",
    "    if stage_key in extracted:\n",
    "        feat = extracted[stage_key]\n",
    "        if feat.dim() == 4:\n",
    "            feat = feat[0]\n",
    "        \n",
    "        # Top row: Mean activation (what CBAM emphasizes)\n",
    "        feat_mean = feat.mean(dim=0).cpu().numpy()\n",
    "        feat_mean = (feat_mean - feat_mean.min()) / (feat_mean.max() - feat_mean.min() + 1e-8)\n",
    "        \n",
    "        axes[0, idx].imshow(feat_mean, cmap='viridis')\n",
    "        axes[0, idx].set_title(f'{stage_name}\\nMean Activation', fontsize=10)\n",
    "        axes[0, idx].axis('off')\n",
    "        \n",
    "        # Bottom row: Resized overlay on image\n",
    "        feat_resized = cv2.resize(feat_mean, (image_np.shape[1], image_np.shape[0]))\n",
    "        overlay = overlay_heatmap(image_np, feat_resized, alpha=0.5, colormap='jet')\n",
    "        \n",
    "        axes[1, idx].imshow(overlay)\n",
    "        axes[1, idx].set_title(f'Attention Overlay', fontsize=10)\n",
    "        axes[1, idx].axis('off')\n",
    "    else:\n",
    "        axes[0, idx].axis('off')\n",
    "        axes[1, idx].axis('off')\n",
    "\n",
    "fig.suptitle('Stage 3: CBAM Attention - Where the Model Focuses at Each Scale', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235bfb11",
   "metadata": {},
   "source": [
    "## Stage 4: Feature Pyramid Network (FPN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad1f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FPN multi-scale features\n",
    "fig = visualize_fpn_features(fpn_features, figsize=(20, 10))\n",
    "fig.suptitle('Stage 4: FPN Multi-Scale Features (P2-P5)', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f63909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPN features overlaid on image\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "for idx, level in enumerate(['P2', 'P3', 'P4', 'P5']):\n",
    "    if level in fpn_features:\n",
    "        feat = fpn_features[level]\n",
    "        if feat.dim() == 4:\n",
    "            feat = feat[0]\n",
    "        \n",
    "        # Mean activation\n",
    "        feat_mean = feat.mean(dim=0).cpu().numpy()\n",
    "        feat_mean = (feat_mean - feat_mean.min()) / (feat_mean.max() - feat_mean.min() + 1e-8)\n",
    "        \n",
    "        # Top row: Raw feature map\n",
    "        axes[0, idx].imshow(feat_mean, cmap='plasma')\n",
    "        axes[0, idx].set_title(f'{level}: {feat.shape[1]}x{feat.shape[2]}\\n({feat.shape[0]} channels)', fontsize=10)\n",
    "        axes[0, idx].axis('off')\n",
    "        \n",
    "        # Bottom row: Overlay on image\n",
    "        feat_resized = cv2.resize(feat_mean, (image_np.shape[1], image_np.shape[0]))\n",
    "        overlay = overlay_heatmap(image_np, feat_resized, alpha=0.5, colormap='plasma')\n",
    "        \n",
    "        axes[1, idx].imshow(overlay)\n",
    "        axes[1, idx].set_title(f'{level} Overlay on Image', fontsize=10)\n",
    "        axes[1, idx].axis('off')\n",
    "\n",
    "fig.suptitle('Stage 4: FPN Feature Maps at Each Scale', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ca2af",
   "metadata": {},
   "source": [
    "## Stage 5: FPN Attention Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa4c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FPN attention outputs\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "fpn_attention_keys = ['fpn_attention_0', 'fpn_attention_1', 'fpn_attention_2', 'fpn_attention_3']\n",
    "fpn_level_names = ['P5 (smallest)', 'P4', 'P3', 'P2 (largest)']\n",
    "\n",
    "for idx, (key, name) in enumerate(zip(fpn_attention_keys, fpn_level_names)):\n",
    "    if key in extracted:\n",
    "        feat = extracted[key]\n",
    "        if feat.dim() == 4:\n",
    "            feat = feat[0]\n",
    "        \n",
    "        feat_mean = feat.mean(dim=0).cpu().numpy()\n",
    "        feat_mean = (feat_mean - feat_mean.min()) / (feat_mean.max() - feat_mean.min() + 1e-8)\n",
    "        \n",
    "        # Top: Feature map\n",
    "        axes[0, idx].imshow(feat_mean, cmap='viridis')\n",
    "        axes[0, idx].set_title(f'{name}\\nAfter Attention', fontsize=10)\n",
    "        axes[0, idx].axis('off')\n",
    "        \n",
    "        # Bottom: Overlay\n",
    "        feat_resized = cv2.resize(feat_mean, (image_np.shape[1], image_np.shape[0]))\n",
    "        overlay = overlay_heatmap(image_np, feat_resized, alpha=0.5)\n",
    "        \n",
    "        axes[1, idx].imshow(overlay)\n",
    "        axes[1, idx].set_title(f'Attention Overlay', fontsize=10)\n",
    "        axes[1, idx].axis('off')\n",
    "    else:\n",
    "        axes[0, idx].axis('off')\n",
    "        axes[1, idx].axis('off')\n",
    "\n",
    "fig.suptitle('Stage 5: FPN Attention Module Outputs', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ec557",
   "metadata": {},
   "source": [
    "## Stage 6: RPN Proposals (Region Proposal Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detected boxes as RPN-like proposals\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# All predictions (before NMS/thresholding)\n",
    "boxes = predictions['boxes'].cpu().numpy()\n",
    "scores = predictions['scores'].cpu().numpy()\n",
    "labels = predictions['labels'].cpu().numpy()\n",
    "\n",
    "# Left: All predictions colored by confidence\n",
    "axes[0].imshow(image_np)\n",
    "cmap = plt.get_cmap('RdYlGn')\n",
    "\n",
    "for box, score in zip(boxes, scores):\n",
    "    x1, y1, x2, y2 = box\n",
    "    color = cmap(score)\n",
    "    rect = patches.Rectangle(\n",
    "        (x1, y1), x2-x1, y2-y1,\n",
    "        linewidth=1, edgecolor=color, facecolor='none', alpha=0.7\n",
    "    )\n",
    "    axes[0].add_patch(rect)\n",
    "\n",
    "axes[0].set_title(f'All Predictions ({len(boxes)} boxes)\\nColored by confidence (red=low, green=high)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Right: High confidence predictions only\n",
    "axes[1].imshow(image_np)\n",
    "\n",
    "high_conf = scores >= CONF_THRESHOLD\n",
    "for box, score, label in zip(boxes[high_conf], scores[high_conf], labels[high_conf]):\n",
    "    x1, y1, x2, y2 = box\n",
    "    color = np.array(ISAID_COLORS.get(label, [255, 255, 255])) / 255.0\n",
    "    \n",
    "    rect = patches.Rectangle(\n",
    "        (x1, y1), x2-x1, y2-y1,\n",
    "        linewidth=2, edgecolor=color, facecolor='none'\n",
    "    )\n",
    "    axes[1].add_patch(rect)\n",
    "    axes[1].text(x1, y1-3, f'{ISAID_CLASS_LABELS[label]}: {score:.2f}',\n",
    "                 fontsize=8, color='white', backgroundcolor=color)\n",
    "\n",
    "axes[1].set_title(f'High Confidence (>{CONF_THRESHOLD}) Predictions ({high_conf.sum()} boxes)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "fig.suptitle('Stage 6: Region Proposals and NMS Results', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6742da2",
   "metadata": {},
   "source": [
    "## Stage 7: RoI Align Sampling Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RoI Align sampling grids\n",
    "if len(predictions['boxes']) > 0:\n",
    "    # Use top 4 predictions\n",
    "    top_k = min(4, len(predictions['boxes']))\n",
    "    top_boxes = predictions['boxes'][:top_k]\n",
    "    \n",
    "    fig = visualize_roi_align_grid(\n",
    "        image_np,\n",
    "        top_boxes,\n",
    "        output_size=7,\n",
    "        sampling_ratio=2,\n",
    "        max_boxes=top_k,\n",
    "        figsize=(20, 6)\n",
    "    )\n",
    "    fig.suptitle('Stage 7: RoI Align - Bilinear Sampling Grid Visualization', fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No detections to visualize RoI Align grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada4e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed RoI Align explanation\n",
    "if len(predictions['boxes']) > 0:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    box_idx = 0\n",
    "    box = predictions['boxes'][box_idx].cpu().numpy()\n",
    "    x1, y1, x2, y2 = box\n",
    "    w, h = x2 - x1, y2 - y1\n",
    "    \n",
    "    output_size = 7\n",
    "    sampling_ratio = 2\n",
    "    \n",
    "    # Row 1: Different aspects of RoI Align\n",
    "    # 1. Original RoI region\n",
    "    crop = image_np[int(y1):int(y2), int(x1):int(x2)]\n",
    "    axes[0, 0].imshow(crop)\n",
    "    axes[0, 0].set_title(f'Original RoI\\n{int(w)}x{int(h)} pixels', fontsize=10)\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # 2. Grid overlay\n",
    "    axes[0, 1].imshow(crop)\n",
    "    cell_w = w / output_size\n",
    "    cell_h = h / output_size\n",
    "    for gx in range(output_size + 1):\n",
    "        x = gx * cell_w\n",
    "        axes[0, 1].axvline(x, color='yellow', linewidth=1)\n",
    "    for gy in range(output_size + 1):\n",
    "        y = gy * cell_h\n",
    "        axes[0, 1].axhline(y, color='yellow', linewidth=1)\n",
    "    axes[0, 1].set_title(f'7x7 Output Grid\\n{output_size*output_size} cells', fontsize=10)\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # 3. Sampling points\n",
    "    axes[0, 2].imshow(crop)\n",
    "    for gx in range(output_size):\n",
    "        for gy in range(output_size):\n",
    "            for sx in range(sampling_ratio):\n",
    "                for sy in range(sampling_ratio):\n",
    "                    px = (gx + (sx + 0.5) / sampling_ratio) * cell_w\n",
    "                    py = (gy + (sy + 0.5) / sampling_ratio) * cell_h\n",
    "                    axes[0, 2].plot(px, py, 'r.', markersize=3)\n",
    "    axes[0, 2].set_title(f'Bilinear Sampling Points\\n{output_size**2 * sampling_ratio**2} points', fontsize=10)\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # 4. Final 7x7 output (from pooled features if available)\n",
    "    axes[0, 3].imshow(cv2.resize(crop, (7*20, 7*20), interpolation=cv2.INTER_NEAREST))\n",
    "    axes[0, 3].set_title('Final 7x7 Output\\n(Bilinear interpolated)', fontsize=10)\n",
    "    axes[0, 3].axis('off')\n",
    "    \n",
    "    # Row 2: Multiple boxes\n",
    "    for i in range(min(4, len(predictions['boxes']))):\n",
    "        box = predictions['boxes'][i].cpu().numpy()\n",
    "        x1, y1, x2, y2 = box\n",
    "        crop = image_np[max(0,int(y1)):int(y2), max(0,int(x1)):int(x2)]\n",
    "        if crop.size > 0:\n",
    "            axes[1, i].imshow(crop)\n",
    "            label = predictions['labels'][i].item()\n",
    "            score = predictions['scores'][i].item()\n",
    "            axes[1, i].set_title(f'RoI {i+1}: {ISAID_CLASS_LABELS[label]}\\nConf: {score:.2f}', fontsize=10)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    fig.suptitle('Stage 7: RoI Align - How Features are Extracted from Each Proposal', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185d96e",
   "metadata": {},
   "source": [
    "## Stage 8: Box Head Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783719b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize box head features\n",
    "if 'box_head_fc1' in extracted and 'box_cls_score' in extracted:\n",
    "    fig = visualize_box_head_features(\n",
    "        extracted['box_head_fc1'],\n",
    "        extracted['box_head_fc2'],\n",
    "        extracted['box_cls_score'],\n",
    "        extracted.get('box_bbox_pred', torch.zeros(1)),\n",
    "        ISAID_CLASS_LABELS,\n",
    "        max_rois=min(8, len(predictions['boxes'])) if len(predictions['boxes']) > 0 else 8,\n",
    "    )\n",
    "    fig.suptitle('Stage 8: Box Head - Classification & Regression', fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Box head features not captured - hooks may not have fired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba162349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed class probability analysis\n",
    "if 'box_cls_score' in extracted and len(predictions['boxes']) > 0:\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    cls_scores = extracted['box_cls_score']\n",
    "    cls_probs = F.softmax(cls_scores, dim=1).cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(min(8, len(cls_probs))):\n",
    "        probs = cls_probs[i]\n",
    "        \n",
    "        # Get top 5 classes\n",
    "        top_indices = np.argsort(probs)[::-1][:5]\n",
    "        top_probs = probs[top_indices]\n",
    "        top_names = [ISAID_CLASS_LABELS[idx][:10] for idx in top_indices]\n",
    "        \n",
    "        colors = [np.array(ISAID_COLORS.get(idx, [128,128,128]))/255.0 for idx in top_indices]\n",
    "        \n",
    "        bars = axes[i].barh(range(5), top_probs, color=colors)\n",
    "        axes[i].set_yticks(range(5))\n",
    "        axes[i].set_yticklabels(top_names)\n",
    "        axes[i].set_xlim(0, 1)\n",
    "        axes[i].set_title(f'RoI {i+1}', fontsize=10)\n",
    "        axes[i].invert_yaxis()\n",
    "        \n",
    "        # Add probability values\n",
    "        for j, (bar, p) in enumerate(zip(bars, top_probs)):\n",
    "            axes[i].text(p + 0.02, j, f'{p:.2f}', va='center', fontsize=8)\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for i in range(min(8, len(cls_probs)), 8):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    fig.suptitle('Stage 8: Per-RoI Class Probability Distribution (Top 5 Classes)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f53668",
   "metadata": {},
   "source": [
    "## Stage 9: Mask Head Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d688b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize mask head intermediate stages\n",
    "mask_features = {k: v for k, v in extracted.items() if k.startswith('mask_head_')}\n",
    "\n",
    "if mask_features and len(predictions['boxes']) > 0:\n",
    "    print(f\"Mask head stages captured: {list(mask_features.keys())}\")\n",
    "    \n",
    "    fig = visualize_mask_head_stages(mask_features, box_idx=0)\n",
    "    fig.suptitle('Stage 9: Mask Head - Convolutional Stages', fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Mask head features not captured or no detections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e8e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize individual mask predictions\n",
    "if 'masks' in predictions and len(predictions['masks']) > 0:\n",
    "    masks = predictions['masks'].cpu().numpy()\n",
    "    \n",
    "    n_masks = min(8, len(masks))\n",
    "    fig, axes = plt.subplots(2, n_masks, figsize=(4*n_masks, 8))\n",
    "    \n",
    "    for i in range(n_masks):\n",
    "        mask = masks[i]\n",
    "        if mask.ndim == 3:\n",
    "            mask = mask[0]  # Remove channel dim\n",
    "        \n",
    "        label = predictions['labels'][i].item()\n",
    "        score = predictions['scores'][i].item()\n",
    "        box = predictions['boxes'][i].cpu().numpy()\n",
    "        \n",
    "        # Top: Raw mask probability\n",
    "        axes[0, i].imshow(mask, cmap='hot', vmin=0, vmax=1)\n",
    "        axes[0, i].set_title(f'{ISAID_CLASS_LABELS[label]}\\nP={score:.2f}', fontsize=10)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Bottom: Binary mask on image crop\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        crop = image_np[max(0,y1):y2, max(0,x1):x2].copy()\n",
    "        mask_binary = (mask > 0.5).astype(np.uint8)\n",
    "        mask_crop = mask_binary[max(0,y1):y2, max(0,x1):x2]\n",
    "        \n",
    "        if crop.size > 0 and mask_crop.size > 0:\n",
    "            # Resize mask to crop size if needed\n",
    "            if mask_crop.shape != crop.shape[:2]:\n",
    "                mask_crop = cv2.resize(mask_crop, (crop.shape[1], crop.shape[0]))\n",
    "            \n",
    "            color = np.array(ISAID_COLORS.get(label, [255, 0, 0]))\n",
    "            overlay = crop.copy().astype(np.float32)\n",
    "            overlay[mask_crop > 0] = overlay[mask_crop > 0] * 0.5 + color * 0.5\n",
    "            overlay = np.clip(overlay, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            axes[1, i].imshow(overlay)\n",
    "        axes[1, i].set_title('Masked Region', fontsize=10)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    fig.suptitle('Stage 9: Individual Mask Predictions', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No mask predictions available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e8ab9",
   "metadata": {},
   "source": [
    "## Stage 10: Grad-CAM Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c433ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Grad-CAM style heatmaps from different FPN levels\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "levels = ['P2', 'P3', 'P4', 'P5']\n",
    "\n",
    "for idx, level in enumerate(levels):\n",
    "    if level in fpn_features:\n",
    "        feat = fpn_features[level]\n",
    "        if feat.dim() == 4:\n",
    "            feat = feat[0]\n",
    "        \n",
    "        # Compute activation-based heatmap (Grad-CAM approximation)\n",
    "        heatmap = feat.mean(dim=0).cpu().numpy()\n",
    "        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "        heatmap = cv2.resize(heatmap, (image_np.shape[1], image_np.shape[0]))\n",
    "        \n",
    "        # Top row: Heatmap only\n",
    "        axes[0, idx].imshow(heatmap, cmap='jet')\n",
    "        axes[0, idx].set_title(f'{level} Activation Map', fontsize=12)\n",
    "        axes[0, idx].axis('off')\n",
    "        \n",
    "        # Bottom row: Overlay on image\n",
    "        overlay = overlay_heatmap(image_np, heatmap, alpha=0.5, colormap='jet')\n",
    "        axes[1, idx].imshow(overlay)\n",
    "        axes[1, idx].set_title(f'{level} Overlay', fontsize=12)\n",
    "        axes[1, idx].axis('off')\n",
    "\n",
    "fig.suptitle('Stage 10: Grad-CAM Heatmaps - Where the Model Looks at Each FPN Level', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2aaf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined multi-scale heatmap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(image_np)\n",
    "axes[0].set_title('Original Image', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Combine heatmaps from multiple scales\n",
    "combined_heatmap = np.zeros((image_np.shape[0], image_np.shape[1]), dtype=np.float32)\n",
    "weights = {'P2': 0.1, 'P3': 0.2, 'P4': 0.3, 'P5': 0.4}  # Higher weight for deeper features\n",
    "\n",
    "for level, weight in weights.items():\n",
    "    if level in fpn_features:\n",
    "        feat = fpn_features[level]\n",
    "        if feat.dim() == 4:\n",
    "            feat = feat[0]\n",
    "        \n",
    "        heatmap = feat.mean(dim=0).cpu().numpy()\n",
    "        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "        heatmap = cv2.resize(heatmap, (image_np.shape[1], image_np.shape[0]))\n",
    "        combined_heatmap += weight * heatmap\n",
    "\n",
    "# Normalize combined\n",
    "combined_heatmap = (combined_heatmap - combined_heatmap.min()) / (combined_heatmap.max() - combined_heatmap.min() + 1e-8)\n",
    "\n",
    "# Multi-scale heatmap\n",
    "axes[1].imshow(combined_heatmap, cmap='jet')\n",
    "axes[1].set_title('Multi-Scale Combined Heatmap', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Overlay\n",
    "overlay = overlay_heatmap(image_np, combined_heatmap, alpha=0.5, colormap='jet')\n",
    "axes[2].imshow(overlay)\n",
    "axes[2].set_title('Combined Heatmap Overlay', fontsize=14)\n",
    "axes[2].axis('off')\n",
    "\n",
    "fig.suptitle('Multi-Scale Grad-CAM: Combined Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb42126",
   "metadata": {},
   "source": [
    "## Stage 11: Final Predictions with Full Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9c664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive visualization\n",
    "fig = visualize_final_predictions(\n",
    "    image_np,\n",
    "    predictions,\n",
    "    gradcam_heatmap=combined_heatmap,\n",
    "    class_labels=ISAID_CLASS_LABELS,\n",
    "    conf_threshold=CONF_THRESHOLD,\n",
    "    figsize=(18, 6)\n",
    ")\n",
    "fig.suptitle('Stage 11: Final Predictions with Masks and Grad-CAM', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detection summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "boxes = predictions['boxes'].cpu().numpy()\n",
    "labels = predictions['labels'].cpu().numpy()\n",
    "scores = predictions['scores'].cpu().numpy()\n",
    "\n",
    "high_conf = scores >= CONF_THRESHOLD\n",
    "print(f\"\\nTotal detections: {len(boxes)}\")\n",
    "print(f\"High confidence (>{CONF_THRESHOLD}): {high_conf.sum()}\")\n",
    "\n",
    "print(f\"\\n{'#':<4} {'Class':<20} {'Confidence':<12} {'Box (x1,y1,x2,y2)'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, (box, label, score) in enumerate(zip(boxes[high_conf], labels[high_conf], scores[high_conf])):\n",
    "    class_name = ISAID_CLASS_LABELS.get(label, f'Class {label}')\n",
    "    box_str = f\"({box[0]:.0f}, {box[1]:.0f}, {box[2]:.0f}, {box[3]:.0f})\"\n",
    "    print(f\"{i+1:<4} {class_name:<20} {score:<12.4f} {box_str}\")\n",
    "\n",
    "# Class distribution\n",
    "if high_conf.sum() > 0:\n",
    "    print(\"\\n Class Distribution:\")\n",
    "    unique_labels, counts = np.unique(labels[high_conf], return_counts=True)\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        class_name = ISAID_CLASS_LABELS.get(label, f'Class {label}')\n",
    "        print(f\"   {class_name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff2489",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Pipeline Summary Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4fd52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary grid\n",
    "fig = pipeline._create_summary_grid(results, combined_heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d7967",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Save All Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675b8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all visualizations to disk\n",
    "SAVE_OUTPUTS = True  # Set to True to save\n",
    "\n",
    "if SAVE_OUTPUTS:\n",
    "    import os\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"Generating and saving all visualizations to {OUTPUT_DIR}/...\")\n",
    "    \n",
    "    figures = pipeline.generate_all_visualizations(\n",
    "        image_np,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        save=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n Saved {len(figures)} visualization files!\")\n",
    "    print(f\"\\nFiles saved:\")\n",
    "    for fname in sorted(os.listdir(OUTPUT_DIR)):\n",
    "        if fname.endswith('.png'):\n",
    "            print(f\"   {fname}\")\n",
    "else:\n",
    "    print(\"Set SAVE_OUTPUTS = True to save visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19ea6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analyze Multiple Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to quickly analyze any image\n",
    "def analyze_image(image_source, save_prefix=None):\n",
    "    \"\"\"\n",
    "    Quick analysis of an image.\n",
    "    \n",
    "    Args:\n",
    "        image_source: Path to image, numpy array, or dataset index (int)\n",
    "        save_prefix: Optional prefix for saving outputs\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    if isinstance(image_source, int):\n",
    "        # Load from dataset\n",
    "        img_tensor, target = val_dataset[image_source]\n",
    "        img_np = denormalize_image(img_tensor)\n",
    "    elif isinstance(image_source, str):\n",
    "        img_np = np.array(Image.open(image_source).convert('RGB'))\n",
    "    else:\n",
    "        img_np = image_source\n",
    "    \n",
    "    # Run pipeline\n",
    "    pipeline.preprocess_image(img_np)\n",
    "    results = pipeline.run_inference_with_gradients()\n",
    "    \n",
    "    # Quick summary\n",
    "    preds = results['predictions']\n",
    "    n_det = (preds['scores'] >= CONF_THRESHOLD).sum().item()\n",
    "    \n",
    "    # Visualization\n",
    "    fpn_feat = results['fpn_features']\n",
    "    heatmap = None\n",
    "    if 'P4' in fpn_feat:\n",
    "        feat = fpn_feat['P4'][0] if fpn_feat['P4'].dim() == 4 else fpn_feat['P4']\n",
    "        heatmap = feat.mean(dim=0).cpu().numpy()\n",
    "        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "        heatmap = cv2.resize(heatmap, (img_np.shape[1], img_np.shape[0]))\n",
    "    \n",
    "    fig = visualize_final_predictions(\n",
    "        img_np, preds, heatmap,\n",
    "        class_labels=ISAID_CLASS_LABELS,\n",
    "        conf_threshold=CONF_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    if save_prefix:\n",
    "        fig.savefig(f'{OUTPUT_DIR}/{save_prefix}_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    print(f\"Found {n_det} objects with confidence >= {CONF_THRESHOLD}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Function `analyze_image()` ready!\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"  analyze_image(5)                    # Analyze dataset sample #5\")\n",
    "print(\"  analyze_image('path/to/image.jpg')  # Analyze image file\")\n",
    "print(\"  analyze_image(image_array)          # Analyze numpy array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze a few more samples\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# for idx in [1, 2, 3]:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Analyzing sample {idx}\")\n",
    "#     print('='*60)\n",
    "#     analyze_image(idx, save_prefix=f'sample_{idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb86221",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook visualized the complete inference pipeline of our Custom Mask R-CNN:\n",
    "\n",
    "| Stage | Component | What We Saw |\n",
    "|-------|-----------|-------------|\n",
    "| 1 | Input | Original image preprocessing |\n",
    "| 2 | Backbone | EfficientNet feature extraction (C2-C5) |\n",
    "| 3 | CBAM | Channel and spatial attention at each scale |\n",
    "| 4 | FPN | Multi-scale feature fusion (P2-P5) |\n",
    "| 5 | FPN Attention | Attention-enhanced FPN features |\n",
    "| 6 | RPN | Region proposals generation |\n",
    "| 7 | RoI Align | Bilinear sampling grid for each proposal |\n",
    "| 8 | Box Head | Classification and bbox regression |\n",
    "| 9 | Mask Head | Instance segmentation prediction |\n",
    "| 10 | Grad-CAM | Where the model \"looks\" to make predictions |\n",
    "| 11 | Output | Final boxes, classes, masks, and confidence scores |\n",
    "\n",
    "**Key Insights:**\n",
    "- CBAM attention helps the model focus on relevant regions\n",
    "- FPN enables detection at multiple scales\n",
    "- Grad-CAM shows discriminative regions for each class\n",
    "- RoI Align preserves spatial information through bilinear sampling"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

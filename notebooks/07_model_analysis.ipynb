{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f9668f",
   "metadata": {},
   "source": [
    "# Model Analysis: Architecture Metrics & Profiling\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook provides comprehensive analysis of the Mask R-CNN model:\n",
    "\n",
    "1. **Parameter Counting**: Total, trainable, and breakdown by component\n",
    "2. **Model Size**: FP32 and FP16 estimates\n",
    "3. **FLOPs Analysis**: Computational complexity for different input sizes\n",
    "4. **Memory Profiling**: GPU memory requirements for training and inference\n",
    "5. **Component Analysis**: Detailed breakdown of model architecture\n",
    "\n",
    "## Usage\n",
    "\n",
    "Set the path to your trained model checkpoint and run all cells to generate metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219562da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(\".\").resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from models.maskrcnn_model import get_custom_maskrcnn\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd4a61",
   "metadata": {},
   "source": [
    "## 1. Configuration & W&B Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Login to W&B\n",
    "try:\n",
    "    # For Kaggle\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "    user_secrets = UserSecretsClient()\n",
    "    wandb_key = user_secrets.get_secret(\"wandb_key\")\n",
    "    wandb.login(key=wandb_key)\n",
    "except:\n",
    "    # For local - assumes you've already run 'wandb login'\n",
    "    wandb.login()\n",
    "\n",
    "print(\"✓ Logged into W&B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a462ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "NUM_CLASSES = 16  # iSAID has 15 classes + background\n",
    "\n",
    "# W&B artifact configuration\n",
    "WANDB_ENTITY = \"marek-olnk-put-pozna-\"\n",
    "WANDB_PROJECT = \"isaid-custom-segmentation\"\n",
    "ARTIFACT_NAME = \"isaid-maskrcnn-final:v0\"\n",
    "\n",
    "# Analysis settings\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "INPUT_SIZES = [(800, 800), (1024, 1024), (512, 512)]  # For FLOPs analysis\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  W&B Artifact: {WANDB_ENTITY}/{WANDB_PROJECT}/{ARTIFACT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a446a13",
   "metadata": {},
   "source": [
    "## 2. Download Model from W&B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab64822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model artifact from W&B\n",
    "print(\"Downloading model from W&B...\")\n",
    "run = wandb.init(project=WANDB_PROJECT, entity=WANDB_ENTITY, job_type=\"analysis\")\n",
    "artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{ARTIFACT_NAME}\", type=\"model\"\n",
    ")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "print(f\"✓ Model downloaded to: {artifact_dir}\")\n",
    "\n",
    "# Set checkpoint path\n",
    "CHECKPOINT_PATH = Path(artifact_dir) / \"best_map_model.pth\"\n",
    "print(f\"  Checkpoint: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5079428",
   "metadata": {},
   "source": [
    "## 3. Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(\"Creating model architecture...\")\n",
    "model = get_custom_maskrcnn(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    pretrained_backbone=False,  # We'll load trained weights\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    print(f\"Loading checkpoint from {CHECKPOINT_PATH}...\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "\n",
    "    # Extract model weights from checkpoint (handles both formats)\n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        # Checkpoint with metadata (epoch, optimizer, etc.)\n",
    "        model_weights = checkpoint[\"model_state_dict\"]\n",
    "        epoch = checkpoint.get(\"epoch\", \"unknown\")\n",
    "        print(f\"  Checkpoint from epoch: {epoch}\")\n",
    "    else:\n",
    "        # Direct state dict\n",
    "        model_weights = checkpoint\n",
    "\n",
    "    model.load_state_dict(model_weights)\n",
    "    print(\"✓ Checkpoint loaded successfully\")\n",
    "else:\n",
    "    print(f\"Warning: Checkpoint not found at {CHECKPOINT_PATH}\")\n",
    "    print(\"Analyzing untrained model architecture\")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  - Backbone: EfficientNet-B0 with CBAM attention\")\n",
    "print(f\"  - FPN: Custom with attention modules\")\n",
    "print(f\"  - Number of classes: {NUM_CLASSES}\")\n",
    "print(\"\\n✓ Model ready for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e736b",
   "metadata": {},
   "source": [
    "## 4. Parameter Counting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count total and trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "\n",
    "    return {\n",
    "        \"total\": total_params,\n",
    "        \"trainable\": trainable_params,\n",
    "        \"non_trainable\": non_trainable_params,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_number(num):\n",
    "    \"\"\"Format large numbers with K/M/B suffixes.\"\"\"\n",
    "    if num >= 1e9:\n",
    "        return f\"{num/1e9:.2f}B\"\n",
    "    elif num >= 1e6:\n",
    "        return f\"{num/1e6:.2f}M\"\n",
    "    elif num >= 1e3:\n",
    "        return f\"{num/1e3:.2f}K\"\n",
    "    else:\n",
    "        return str(num)\n",
    "\n",
    "\n",
    "# Count parameters\n",
    "param_counts = count_parameters(model)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PARAMETER COUNTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\n",
    "    f\"Total Parameters:        {param_counts['total']:,} ({format_number(param_counts['total'])})\"\n",
    ")\n",
    "print(\n",
    "    f\"Trainable Parameters:    {param_counts['trainable']:,} ({format_number(param_counts['trainable'])})\"\n",
    ")\n",
    "print(\n",
    "    f\"Non-trainable Parameters: {param_counts['non_trainable']:,} ({format_number(param_counts['non_trainable'])})\"\n",
    ")\n",
    "print(f\"\\nTrainable Ratio: {param_counts['trainable']/param_counts['total']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80c675f",
   "metadata": {},
   "source": [
    "## 5. Parameter Breakdown by Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef58332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_component_parameters(model):\n",
    "    \"\"\"\n",
    "    Analyze parameters breakdown by model component.\n",
    "    \"\"\"\n",
    "    component_params = OrderedDict()\n",
    "\n",
    "    # Backbone (excluding FPN if it's a child of backbone)\n",
    "    if hasattr(model, \"backbone\"):\n",
    "        # Get all backbone parameters\n",
    "        backbone_params = sum(p.numel() for p in model.backbone.parameters())\n",
    "\n",
    "        # Check if FPN is part of backbone\n",
    "        fpn_params = 0\n",
    "        if hasattr(model.backbone, \"fpn\"):\n",
    "            fpn_params = sum(p.numel() for p in model.backbone.fpn.parameters())\n",
    "\n",
    "        # Backbone excluding FPN\n",
    "        backbone_only_params = backbone_params - fpn_params\n",
    "        component_params[\"Backbone (excl. FPN)\"] = backbone_only_params\n",
    "\n",
    "        # Try to detect CBAM within backbone\n",
    "        cbam_params = 0\n",
    "        for name, module in model.backbone.named_modules():\n",
    "            if \"cbam\" in name.lower() or \"attention\" in name.lower():\n",
    "                cbam_params += sum(p.numel() for p in module.parameters())\n",
    "        if cbam_params > 0:\n",
    "            component_params[\"  - CBAM Modules\"] = cbam_params\n",
    "            component_params[\"  - Backbone (base)\"] = backbone_only_params - cbam_params\n",
    "\n",
    "    # FPN\n",
    "    if hasattr(model, \"backbone\") and hasattr(model.backbone, \"fpn\"):\n",
    "        fpn_params = sum(p.numel() for p in model.backbone.fpn.parameters())\n",
    "        component_params[\"FPN\"] = fpn_params\n",
    "\n",
    "    # RPN\n",
    "    if hasattr(model, \"rpn\"):\n",
    "        rpn_params = sum(p.numel() for p in model.rpn.parameters())\n",
    "        component_params[\"RPN\"] = rpn_params\n",
    "\n",
    "    # ROI Heads\n",
    "    if hasattr(model, \"roi_heads\"):\n",
    "        roi_heads = model.roi_heads\n",
    "\n",
    "        # Box Head (includes box predictor)\n",
    "        if hasattr(roi_heads, \"box_head\"):\n",
    "            box_head_params = sum(p.numel() for p in roi_heads.box_head.parameters())\n",
    "            component_params[\"Box Head\"] = box_head_params\n",
    "\n",
    "        # Box Predictor (only if separate from box_head)\n",
    "        if hasattr(roi_heads, \"box_predictor\") and hasattr(\n",
    "            roi_heads.box_predictor, \"parameters\"\n",
    "        ):\n",
    "            try:\n",
    "                box_pred_params = sum(\n",
    "                    p.numel() for p in roi_heads.box_predictor.parameters()\n",
    "                )\n",
    "                component_params[\"Box Predictor\"] = box_pred_params\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Mask Head (includes mask predictor)\n",
    "        if hasattr(roi_heads, \"mask_head\"):\n",
    "            mask_head_params = sum(p.numel() for p in roi_heads.mask_head.parameters())\n",
    "            component_params[\"Mask Head\"] = mask_head_params\n",
    "\n",
    "    return component_params\n",
    "\n",
    "\n",
    "# Analyze components\n",
    "component_params = analyze_component_parameters(model)\n",
    "total_params = param_counts[\"total\"]\n",
    "\n",
    "# Create DataFrame\n",
    "records = []\n",
    "for component, params in component_params.items():\n",
    "    records.append(\n",
    "        {\n",
    "            \"Component\": component,\n",
    "            \"Parameters\": params,\n",
    "            \"Formatted\": format_number(params),\n",
    "            \"Percentage\": f\"{params/total_params*100:.2f}%\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "component_df = pd.DataFrame(records)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PARAMETER BREAKDOWN BY COMPONENT\")\n",
    "print(\"=\" * 60)\n",
    "print(component_df.to_string(index=False))\n",
    "print(f\"\\nTotal: {format_number(total_params)} (100%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b434834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Bar chart\n",
    "# Filter out sub-components (those starting with spaces)\n",
    "main_components = component_df[~component_df[\"Component\"].str.startswith(\" \")].copy()\n",
    "\n",
    "ax1.barh(\n",
    "    main_components[\"Component\"],\n",
    "    main_components[\"Parameters\"],\n",
    "    color=\"steelblue\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax1.set_xlabel(\"Number of Parameters\")\n",
    "ax1.set_title(\"Parameters by Component\")\n",
    "ax1.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (comp, params, formatted) in enumerate(\n",
    "    zip(\n",
    "        main_components[\"Component\"],\n",
    "        main_components[\"Parameters\"],\n",
    "        main_components[\"Formatted\"],\n",
    "    )\n",
    "):\n",
    "    ax1.text(params, i, f\" {formatted}\", va=\"center\", fontsize=9)\n",
    "\n",
    "# Pie chart\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(main_components)))\n",
    "wedges, texts, autotexts = ax2.pie(\n",
    "    main_components[\"Parameters\"],\n",
    "    labels=None,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    colors=colors,\n",
    "    pctdistance=0.85,\n",
    ")\n",
    "ax2.set_title(\"Parameter Distribution\", pad=20)\n",
    "\n",
    "# Make percentage text readable\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color(\"white\")\n",
    "    autotext.set_fontsize(10)\n",
    "    autotext.set_weight(\"bold\")\n",
    "\n",
    "# Add legend\n",
    "ax2.legend(\n",
    "    wedges,\n",
    "    main_components[\"Component\"],\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0, 0.5, 1),\n",
    "    fontsize=10,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33a6f4",
   "metadata": {},
   "source": [
    "## 6. Model Size Estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648bfed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model_size(model, precision=\"fp32\"):\n",
    "    \"\"\"\n",
    "    Estimate model size in different precisions.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        precision: 'fp32', 'fp16', or 'int8'\n",
    "\n",
    "    Returns:\n",
    "        Size in bytes\n",
    "    \"\"\"\n",
    "    bytes_per_param = {\"fp32\": 4, \"fp16\": 2, \"int8\": 1}\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_bytes = total_params * bytes_per_param[precision]\n",
    "\n",
    "    return size_bytes\n",
    "\n",
    "\n",
    "def format_bytes(size_bytes):\n",
    "    \"\"\"Format bytes to human-readable format.\"\"\"\n",
    "    if size_bytes >= 1e9:\n",
    "        return f\"{size_bytes/1e9:.2f} GB\"\n",
    "    elif size_bytes >= 1e6:\n",
    "        return f\"{size_bytes/1e6:.2f} MB\"\n",
    "    elif size_bytes >= 1e3:\n",
    "        return f\"{size_bytes/1e3:.2f} KB\"\n",
    "    else:\n",
    "        return f\"{size_bytes} bytes\"\n",
    "\n",
    "\n",
    "# Calculate sizes\n",
    "size_fp32 = estimate_model_size(model, \"fp32\")\n",
    "size_fp16 = estimate_model_size(model, \"fp16\")\n",
    "size_int8 = estimate_model_size(model, \"int8\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL SIZE ESTIMATES (Parameters Only)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"FP32 (32-bit floating point): {format_bytes(size_fp32)}\")\n",
    "print(f\"FP16 (16-bit floating point): {format_bytes(size_fp16)}\")\n",
    "print(f\"INT8 (8-bit integer):         {format_bytes(size_int8)}\")\n",
    "print(f\"\\nCompression Ratios:\")\n",
    "print(f\"  FP16 vs FP32: {size_fp32/size_fp16:.1f}x smaller\")\n",
    "print(f\"  INT8 vs FP32: {size_fp32/size_int8:.1f}x smaller\")\n",
    "\n",
    "# Compare with actual checkpoint file size\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    actual_size = CHECKPOINT_PATH.stat().st_size\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"CHECKPOINT FILE SIZE COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Actual checkpoint file:      {format_bytes(actual_size)}\")\n",
    "    print(f\"Calculated parameters only:  {format_bytes(size_fp32)}\")\n",
    "    print(f\"Overhead:                    {format_bytes(actual_size - size_fp32)}\")\n",
    "    print(f\"\\nNote: Checkpoint includes optimizer state (momentum, variance),\")\n",
    "    print(f\"      training metadata (epoch, losses), and BatchNorm statistics.\")\n",
    "    print(f\"      Optimizer state typically adds ~1-2x the parameter size.\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "precisions = [\"FP32\", \"FP16\", \"INT8\"]\n",
    "sizes_mb = [size_fp32 / 1e6, size_fp16 / 1e6, size_int8 / 1e6]\n",
    "colors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\"]\n",
    "\n",
    "bars = ax.bar(precisions, sizes_mb, color=colors, alpha=0.8)\n",
    "ax.set_ylabel(\"Model Size (MB)\")\n",
    "ax.set_title(\"Model Size by Precision\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, size in zip(bars, sizes_mb):\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height,\n",
    "        f\"{size:.1f} MB\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=11,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385b53c",
   "metadata": {},
   "source": [
    "## 7. FLOPs Analysis\n",
    "\n",
    "Computational complexity for different input sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbb9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from thop import profile, clever_format\n",
    "\n",
    "    THOP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: 'thop' package not installed. Installing...\")\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"thop\"])\n",
    "    from thop import profile, clever_format\n",
    "\n",
    "    THOP_AVAILABLE = True\n",
    "\n",
    "\n",
    "def estimate_flops(model, input_size):\n",
    "    \"\"\"\n",
    "    Estimate FLOPs for a given input size.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        input_size: Tuple of (height, width)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with FLOPs and parameter counts\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    h, w = input_size\n",
    "\n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randn(1, 3, h, w).to(next(model.parameters()).device)\n",
    "\n",
    "    # Note: For detection models, we need to handle the output format\n",
    "    # Some models return dictionaries during training but lists during eval\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Try profiling - this may not work for all detection models\n",
    "            flops, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "            flops_str, params_str = clever_format([flops, params], \"%.3f\")\n",
    "\n",
    "            return {\n",
    "                \"input_size\": f\"{h}×{w}\",\n",
    "                \"flops\": flops,\n",
    "                \"flops_str\": flops_str,\n",
    "                \"params\": params,\n",
    "                \"params_str\": params_str,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not profile for {h}×{w}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if THOP_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FLOPs ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Note: For detection models, FLOPs estimation may be approximate.\\n\")\n",
    "\n",
    "    flops_results = []\n",
    "    for input_size in INPUT_SIZES:\n",
    "        print(f\"Analyzing {input_size[0]}×{input_size[1]}...\")\n",
    "        result = estimate_flops(model, input_size)\n",
    "        if result:\n",
    "            flops_results.append(result)\n",
    "            print(f\"  FLOPs: {result['flops_str']}\")\n",
    "\n",
    "    if flops_results:\n",
    "        # Create summary table\n",
    "        flops_df = pd.DataFrame(flops_results)\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"FLOPS SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(flops_df[[\"input_size\", \"flops_str\"]].to_string(index=False))\n",
    "\n",
    "        # Visualize FLOPs vs input size\n",
    "        if len(flops_results) > 1:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            input_sizes_str = [r[\"input_size\"] for r in flops_results]\n",
    "            flops_gflops = [r[\"flops\"] / 1e9 for r in flops_results]\n",
    "\n",
    "            bars = ax.bar(input_sizes_str, flops_gflops, color=\"coral\", alpha=0.8)\n",
    "            ax.set_ylabel(\"GFLOPs\")\n",
    "            ax.set_xlabel(\"Input Size\")\n",
    "            ax.set_title(\"Computational Complexity by Input Size\")\n",
    "            ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "            # Add value labels\n",
    "            for bar, gflops in zip(bars, flops_gflops):\n",
    "                height = bar.get_height()\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width() / 2.0,\n",
    "                    height,\n",
    "                    f\"{gflops:.1f}\\nGFLOPs\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=10,\n",
    "                )\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"\\nSkipping FLOPs analysis (thop not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb9a8f",
   "metadata": {},
   "source": [
    "## 8. Memory Profiling\n",
    "\n",
    "Estimate GPU memory requirements for different batch sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_memory_usage(model, batch_size=1, input_size=(800, 800), amp=False):\n",
    "    \"\"\"\n",
    "    Estimate GPU memory usage during inference.\n",
    "\n",
    "    Note: This is an approximation. Actual memory usage during training\n",
    "    will be higher due to gradients, optimizer states, etc.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available - skipping GPU memory profiling\")\n",
    "        return None\n",
    "\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    h, w = input_size\n",
    "    dummy_input = [torch.randn(3, h, w).cuda() for _ in range(batch_size)]\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            if amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    _ = model(dummy_input)\n",
    "            else:\n",
    "                _ = model(dummy_input)\n",
    "\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1024**3  # Convert to GB\n",
    "        return peak_memory\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during memory profiling: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GPU MEMORY PROFILING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Estimating inference memory usage...\\n\")\n",
    "\n",
    "    memory_results = []\n",
    "    batch_sizes = [1, 2, 4, 8]\n",
    "\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"Testing batch_size={bs}...\")\n",
    "\n",
    "        # FP32\n",
    "        mem_fp32 = estimate_memory_usage(model, batch_size=bs, amp=False)\n",
    "        if mem_fp32:\n",
    "            print(f\"  FP32: {mem_fp32:.2f} GB\")\n",
    "\n",
    "        # AMP (FP16)\n",
    "        mem_amp = estimate_memory_usage(model, batch_size=bs, amp=True)\n",
    "        if mem_amp:\n",
    "            print(f\"  AMP:  {mem_amp:.2f} GB\")\n",
    "\n",
    "        if mem_fp32 or mem_amp:\n",
    "            memory_results.append(\n",
    "                {\n",
    "                    \"Batch Size\": bs,\n",
    "                    \"FP32 (GB)\": f\"{mem_fp32:.2f}\" if mem_fp32 else \"N/A\",\n",
    "                    \"AMP (GB)\": f\"{mem_amp:.2f}\" if mem_amp else \"N/A\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if memory_results:\n",
    "        memory_df = pd.DataFrame(memory_results)\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"MEMORY USAGE SUMMARY (Inference)\")\n",
    "        print(\"=\" * 60)\n",
    "        print(memory_df.to_string(index=False))\n",
    "        print(\n",
    "            \"\\nNote: Training memory will be ~2-3x higher due to gradients and optimizer states.\"\n",
    "        )\n",
    "\n",
    "        # Visualize\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        x = np.arange(len(batch_sizes))\n",
    "        width = 0.35\n",
    "\n",
    "        fp32_mem = [\n",
    "            float(r[\"FP32 (GB)\"]) if r[\"FP32 (GB)\"] != \"N/A\" else 0\n",
    "            for r in memory_results\n",
    "        ]\n",
    "        amp_mem = [\n",
    "            float(r[\"AMP (GB)\"]) if r[\"AMP (GB)\"] != \"N/A\" else 0\n",
    "            for r in memory_results\n",
    "        ]\n",
    "\n",
    "        ax.bar(x - width / 2, fp32_mem, width, label=\"FP32\", color=\"#e74c3c\", alpha=0.8)\n",
    "        ax.bar(\n",
    "            x + width / 2,\n",
    "            amp_mem,\n",
    "            width,\n",
    "            label=\"AMP (FP16)\",\n",
    "            color=\"#3498db\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "        ax.set_ylabel(\"GPU Memory (GB)\")\n",
    "        ax.set_xlabel(\"Batch Size\")\n",
    "        ax.set_title(\"Inference Memory Usage by Batch Size\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(batch_sizes)\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nCUDA not available - skipping GPU memory profiling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24049f",
   "metadata": {},
   "source": [
    "## 9. Summary Report\n",
    "\n",
    "Generate a formatted summary suitable for copying to the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5bfc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### 5.1 Model Size\\n\")\n",
    "print(\"| Metric                   | Value       |\")\n",
    "print(\"| ------------------------ | ----------- |\")\n",
    "print(f\"| **Total Parameters**     | {format_number(param_counts['total'])} |\")\n",
    "print(f\"| **Trainable Parameters** | {format_number(param_counts['trainable'])} |\")\n",
    "print(f\"| **Model Size (FP32)**    | {format_bytes(size_fp32)} |\")\n",
    "print(f\"| **Model Size (FP16)**    | {format_bytes(size_fp16)} |\")\n",
    "\n",
    "print(\"\\n### 5.2 Parameter Breakdown by Component\\n\")\n",
    "print(\"| Component                  | Parameters | Percentage |\")\n",
    "print(\"| -------------------------- | ---------- | ---------- |\")\n",
    "for _, row in component_df.iterrows():\n",
    "    # Clean up component name for table\n",
    "    comp_name = row[\"Component\"].replace(\"  - \", \"  └─ \")\n",
    "    print(f\"| {comp_name:<26} | {row['Formatted']:<10} | {row['Percentage']:<10} |\")\n",
    "print(\n",
    "    f\"| **Total**                  | {format_number(total_params):<10} | 100%       |\"\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available() and memory_results:\n",
    "    print(\"\\n### 5.3 Memory Requirements\\n\")\n",
    "    print(\"| Configuration                | GPU Memory |\")\n",
    "    print(\"| ---------------------------- | ---------- |\")\n",
    "    for result in memory_results:\n",
    "        bs = result[\"Batch Size\"]\n",
    "        amp = result[\"AMP (GB)\"]\n",
    "        if amp != \"N/A\":\n",
    "            print(f\"| Training (batch_size={bs}, AMP) | ~{amp} GB |\")\n",
    "    print(f\"| Inference (single image)     | ~{memory_results[0]['AMP (GB)']} GB |\")\n",
    "\n",
    "if THOP_AVAILABLE and flops_results:\n",
    "    print(\"\\n### 5.4 FLOPs Analysis\\n\")\n",
    "    print(\"| Input Size | FLOPs      |\")\n",
    "    print(\"| ---------- | ---------- |\")\n",
    "    for result in flops_results:\n",
    "        print(f\"| {result['input_size']:<10} | ~{result['flops_str']:<10} |\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Copy the tables above to your REPORT.md file\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd754fe",
   "metadata": {},
   "source": [
    "## 10. Export Results\n",
    "\n",
    "Save analysis results for future reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create output directory\n",
    "output_dir = project_root / \"analysis_results\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prepare export data\n",
    "export_data = {\n",
    "    \"model_config\": {\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "        \"wandb_artifact\": f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{ARTIFACT_NAME}\",\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"total\": int(param_counts[\"total\"]),\n",
    "        \"trainable\": int(param_counts[\"trainable\"]),\n",
    "        \"non_trainable\": int(param_counts[\"non_trainable\"]),\n",
    "        \"total_formatted\": format_number(param_counts[\"total\"]),\n",
    "        \"trainable_formatted\": format_number(param_counts[\"trainable\"]),\n",
    "    },\n",
    "    \"component_breakdown\": [\n",
    "        {\n",
    "            \"component\": row[\"Component\"],\n",
    "            \"parameters\": int(row[\"Parameters\"]),\n",
    "            \"formatted\": row[\"Formatted\"],\n",
    "            \"percentage\": row[\"Percentage\"],\n",
    "        }\n",
    "        for _, row in component_df.iterrows()\n",
    "    ],\n",
    "    \"model_size\": {\n",
    "        \"fp32_bytes\": int(size_fp32),\n",
    "        \"fp16_bytes\": int(size_fp16),\n",
    "        \"int8_bytes\": int(size_int8),\n",
    "        \"fp32_formatted\": format_bytes(size_fp32),\n",
    "        \"fp16_formatted\": format_bytes(size_fp16),\n",
    "        \"int8_formatted\": format_bytes(size_int8),\n",
    "    },\n",
    "}\n",
    "\n",
    "if THOP_AVAILABLE and flops_results:\n",
    "    export_data[\"flops\"] = [\n",
    "        {\n",
    "            \"input_size\": r[\"input_size\"],\n",
    "            \"flops\": int(r[\"flops\"]),\n",
    "            \"flops_formatted\": r[\"flops_str\"],\n",
    "        }\n",
    "        for r in flops_results\n",
    "    ]\n",
    "\n",
    "if torch.cuda.is_available() and memory_results:\n",
    "    export_data[\"memory_usage\"] = memory_results\n",
    "\n",
    "# Save to JSON\n",
    "output_file = output_dir / \"model_analysis.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Analysis results saved to: {output_file}\")\n",
    "\n",
    "# Also save component breakdown as CSV\n",
    "component_df.to_csv(output_dir / \"model_components.csv\", index=False)\n",
    "print(f\"✓ Component breakdown saved to: {output_dir / 'model_components.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b13e20",
   "metadata": {},
   "source": [
    "# Training with Weights & Biases Integration\n",
    "\n",
    "This notebook demonstrates iSAID instance segmentation training with full W&B tracking using the integrated `Trainer` class.\n",
    "\n",
    "**Features:**\n",
    "- Automatic logging of training/validation losses and metrics\n",
    "- Gradient norm tracking for CBAM and RoI layers  \n",
    "- Learning rate scheduling (OneCycleLR or ReduceLROnPlateau)\n",
    "- Validation predictions visualization\n",
    "- Model checkpointing as W&B artifacts\n",
    "- mAP, mean IoU, and overfitting gap metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc7359",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1325a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michaelo-ponteski/isaid-instance-segmentation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0951f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/isaid-instance-segmentation\n",
    "!git pull\n",
    "!git switch wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd4af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Set memory optimization for CUDA\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7312cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60075723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wandb if not available\n",
    "try:\n",
    "    import wandb\n",
    "    print(f\"wandb version: {wandb.__version__}\") # Must be newest\n",
    "except ImportError:\n",
    "    print(\"Installing wandb...\")\n",
    "    !pip install --upgrade wandb\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e6bef",
   "metadata": {},
   "source": [
    "### Kaggle wandb API setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ee638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# 1. Retrieve the key securely\n",
    "user_secrets = UserSecretsClient()\n",
    "my_secret = user_secrets.get_secret(\"wandb_key\") \n",
    "\n",
    "# 2. Login explicitly (bypasses the interactive freeze)\n",
    "wandb.login(key=my_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import datasets.isaid_dataset\n",
    "import models.maskrcnn_model\n",
    "import training.transforms\n",
    "import training.trainer\n",
    "\n",
    "importlib.reload(datasets.isaid_dataset)\n",
    "importlib.reload(models.maskrcnn_model)\n",
    "importlib.reload(training.transforms)\n",
    "importlib.reload(training.trainer)\n",
    "\n",
    "from datasets.isaid_dataset import iSAIDDataset\n",
    "from training.transforms import get_transforms\n",
    "from training.trainer import Trainer, create_datasets\n",
    "from models.maskrcnn_model import CustomMaskRCNN\n",
    "from training.wandb_logger import ISAID_CLASS_LABELS\n",
    "\n",
    "print(\"All modules imported successfully!\")\n",
    "print(f\"\\niSAID Class Labels:\")\n",
    "for idx, name in ISAID_CLASS_LABELS.items():\n",
    "    print(f\"  {idx}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10009ac5",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ddaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All hyperparameters in one place - this will be logged to W&B\n",
    "HYPERPARAMETERS = {\n",
    "    # Dataset\n",
    "    \"data_root\": \"/kaggle/input/isaid-patches/iSAID_patches\",\n",
    "    \"num_classes\": 16,\n",
    "    \"image_size\": 800,\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 2,\n",
    "    \"num_epochs\": 20,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"momentum\": 0.9,\n",
    "    \n",
    "    # Model Architecture\n",
    "    \"backbone\": \"efficientnet_b0\",\n",
    "    \"pretrained_backbone\": True,\n",
    "    \"cbam_reduction_ratio\": 16,\n",
    "    \"roi_head_layers\": 4,\n",
    "    \n",
    "    # RPN Anchors (optimized for iSAID)\n",
    "    \"anchor_sizes\": ((8, 16), (16, 32), (32, 64), (64, 128)),\n",
    "    \"aspect_ratios\": ((0.5, 1.0, 2.0),) * 4,\n",
    "    \n",
    "    # W&B Logging\n",
    "    \"wandb_project\": \"isaid-custom-segmentation\",\n",
    "    \"wandb_entity\": \"marek-olnk-put-pozna-\",\n",
    "    \"wandb_log_freq\": 20,  # Log every N batches\n",
    "    \"wandb_num_val_images\": 4,  # Number of images for validation visualization\n",
    "    \"wandb_conf_threshold\": 0.5,  # Confidence threshold for predictions\n",
    "}\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "for k, v in HYPERPARAMETERS.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639732e6",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d35a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset, val_dataset = create_datasets(\n",
    "    data_root=HYPERPARAMETERS[\"data_root\"],\n",
    "    image_size=HYPERPARAMETERS[\"image_size\"],\n",
    "    subset_fraction=1.0,  # Use full dataset\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7905fc",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with custom anchor configuration\n",
    "model = CustomMaskRCNN(\n",
    "    num_classes=HYPERPARAMETERS[\"num_classes\"],\n",
    "    pretrained_backbone=HYPERPARAMETERS[\"pretrained_backbone\"],\n",
    "    rpn_anchor_sizes=HYPERPARAMETERS[\"anchor_sizes\"],\n",
    "    rpn_aspect_ratios=HYPERPARAMETERS[\"aspect_ratios\"],\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac29056",
   "metadata": {},
   "source": [
    "## 5. Create Trainer with W&B Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer with W&B integration\n",
    "# The trainer handles all logging automatically!\n",
    "trainer = Trainer(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    model=model,\n",
    "    batch_size=HYPERPARAMETERS[\"batch_size\"],\n",
    "    lr=HYPERPARAMETERS[\"learning_rate\"],\n",
    "    device=device,\n",
    "    use_amp=True,\n",
    "    num_workers=4,\n",
    "    # W&B configuration\n",
    "    wandb_project=HYPERPARAMETERS[\"wandb_project\"],\n",
    "    wandb_entity=HYPERPARAMETERS[\"entity\"],\n",
    "    wandb_tags=[\"maskrcnn\", \"efficientnet\", \"cbam\", \"trainer-integrated\"],\n",
    "    wandb_notes=\"Training with integrated Trainer class - EfficientNet backbone + CBAM + FPN\",\n",
    "    wandb_log_freq=HYPERPARAMETERS[\"wandb_log_freq\"],\n",
    "    wandb_num_val_images=HYPERPARAMETERS[\"wandb_num_val_images\"],\n",
    "    wandb_conf_threshold=HYPERPARAMETERS[\"wandb_conf_threshold\"],\n",
    "    hyperparameters=HYPERPARAMETERS,\n",
    ")\n",
    "\n",
    "print(f\"\\nW&B Run: {trainer.wandb_logger.run.name}\")\n",
    "print(f\"URL: {trainer.wandb_logger.run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e100397",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "The `Trainer.fit()` method handles everything:\n",
    "- Training loop with gradient clipping and AMP\n",
    "- Validation loss computation\n",
    "- mAP and mean IoU metrics\n",
    "- W&B logging (losses, gradients, predictions, checkpoints)\n",
    "- Learning rate scheduling\n",
    "- Best model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42497ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training!\n",
    "# All W&B logging happens automatically inside trainer.fit()\n",
    "history = trainer.fit(\n",
    "    epochs=HYPERPARAMETERS[\"num_epochs\"],\n",
    "    save_dir=\"checkpoints\",\n",
    "    compute_metrics_every=1,  # Compute mAP every epoch\n",
    "    max_map_samples=200,  # Limit samples for faster mAP computation\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a3ea8",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b69d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history[\"train/loss\"], label=\"Train Loss\")\n",
    "ax.plot(history[\"val/loss\"], label=\"Val Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training & Validation Loss\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# mAP curves\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history[\"train/mAP@0.5\"], label=\"Train mAP@0.5\")\n",
    "ax.plot(history[\"val/mAP@0.5\"], label=\"Val mAP@0.5\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"mAP@0.5\")\n",
    "ax.set_title(\"mAP Performance\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history[\"train/lr\"])\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Learning Rate\")\n",
    "ax.set_title(\"Learning Rate Schedule\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history[\"train/grad_norm\"])\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Gradient Norm\")\n",
    "ax.set_title(\"Training Gradient Norm\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8168e47",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e23555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on validation set\n",
    "trainer.visualize_predictions(\n",
    "    num_samples=5,\n",
    "    score_threshold=0.5,\n",
    "    mask_alpha=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef58701",
   "metadata": {},
   "source": [
    "## 9. Finish W&B Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe784e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the W&B run\n",
    "trainer.finish()\n",
    "\n",
    "print(f\"\\nW&B run completed!\")\n",
    "print(f\"View results at: {trainer.wandb_logger.run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a1973",
   "metadata": {},
   "source": [
    "## 10. Load Model from W&B Artifact (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load best model from W&B artifacts\n",
    "# Uncomment to use\n",
    "\n",
    "# import wandb\n",
    "# api = wandb.Api()\n",
    "# artifact = api.artifact('YOUR_ENTITY/isaid-custom-segmentation/isaid-model:best')\n",
    "# artifact_dir = artifact.download()\n",
    "# \n",
    "# model = CustomMaskRCNN(num_classes=16)\n",
    "# model.load_state_dict(torch.load(f\"{artifact_dir}/best_model.pth\"))\n",
    "# model.eval()\n",
    "# print(\"Model loaded from W&B artifact!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

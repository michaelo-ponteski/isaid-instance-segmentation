{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b13e20",
   "metadata": {},
   "source": [
    "# Training with Weights & Biases Integration\n",
    "\n",
    "This notebook demonstrates full W&B tracking for iSAID instance segmentation training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc7359",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1325a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michaelo-ponteski/isaid-instance-segmentation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0951f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/isaid-instance-segmentation\n",
    "!git pull\n",
    "!git switch wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd4af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Set memory optimization for CUDA\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60075723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wandb if not available\n",
    "try:\n",
    "    import wandb\n",
    "    print(f\"wandb version: {wandb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing wandb...\")\n",
    "    !pip install wandb\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ee638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to W&B (run once)\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import datasets.isaid_dataset\n",
    "import models.maskrcnn_model\n",
    "import training.transforms\n",
    "\n",
    "importlib.reload(datasets.isaid_dataset)\n",
    "importlib.reload(models.maskrcnn_model)\n",
    "importlib.reload(training.transforms)\n",
    "\n",
    "from datasets.isaid_dataset import iSAIDDataset\n",
    "from training.transforms import get_transforms\n",
    "from training.wandb_logger import (\n",
    "    WandbLogger,\n",
    "    WandbConfig,\n",
    "    create_wandb_logger,\n",
    "    compute_gradient_norms,\n",
    "    get_fixed_val_batch,\n",
    "    ISAID_CLASS_LABELS,\n",
    ")\n",
    "from models.maskrcnn_model import CustomMaskRCNN, get_custom_maskrcnn\n",
    "\n",
    "print(\"All modules imported successfully!\")\n",
    "print(f\"\\niSAID Class Labels:\")\n",
    "for idx, name in ISAID_CLASS_LABELS.items():\n",
    "    print(f\"  {idx}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10009ac5",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ddaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All hyperparameters in one place - this will be logged to W&B\n",
    "HYPERPARAMETERS = {\n",
    "    # Dataset\n",
    "    \"data_root\": \"/kaggle/input/isaid-patches/iSAID_patches\",\n",
    "    \"num_classes\": 16,\n",
    "    \"image_size\": 800,\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 2,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"momentum\": 0.9,\n",
    "    \n",
    "    # Model Architecture\n",
    "    \"backbone\": \"efficientnet_b0\",\n",
    "    \"pretrained_backbone\": True,\n",
    "    \"cbam_reduction_ratio\": 16,\n",
    "    \"roi_head_layers\": 4,\n",
    "    \n",
    "    # RPN Anchors\n",
    "    \"anchor_sizes\": ((8, 16), (16, 32), (32, 64), (64, 128)),\n",
    "    \"aspect_ratios\": ((0.5, 1.0, 2.0),) * 4,\n",
    "    \n",
    "    # Scheduler\n",
    "    \"scheduler_type\": \"onecycle\",\n",
    "    \"max_lr\": 0.01,\n",
    "    \n",
    "    # W&B Logging\n",
    "    \"log_freq\": 20,  # Log every N batches\n",
    "    \"num_val_images\": 4,  # Number of images for validation visualization\n",
    "    \"conf_threshold\": 0.5,  # Confidence threshold for predictions\n",
    "}\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "for k, v in HYPERPARAMETERS.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639732e6",
   "metadata": {},
   "source": [
    "## 3. Initialize W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d35a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create W&B logger\n",
    "wandb_config = WandbConfig(\n",
    "    project=\"isaid-custom-segmentation\",\n",
    "    run_name=None,  # Auto-generated, or set a custom name\n",
    "    tags=[\"maskrcnn\", \"efficientnet\", \"cbam\"],\n",
    "    notes=\"Training with custom EfficientNet backbone + CBAM + FPN\",\n",
    "    log_freq=HYPERPARAMETERS[\"log_freq\"],\n",
    "    num_val_images=HYPERPARAMETERS[\"num_val_images\"],\n",
    "    conf_threshold=HYPERPARAMETERS[\"conf_threshold\"],\n",
    ")\n",
    "\n",
    "logger = WandbLogger(wandb_config, HYPERPARAMETERS)\n",
    "print(f\"\\nW&B Run: {logger.run.name}\")\n",
    "print(f\"URL: {logger.run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7905fc",
   "metadata": {},
   "source": [
    "## 4. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "train_dataset = iSAIDDataset(\n",
    "    HYPERPARAMETERS[\"data_root\"],\n",
    "    split=\"train\",\n",
    "    transforms=get_transforms(train=True),\n",
    "    image_size=HYPERPARAMETERS[\"image_size\"],\n",
    ")\n",
    "\n",
    "val_dataset = iSAIDDataset(\n",
    "    HYPERPARAMETERS[\"data_root\"],\n",
    "    split=\"val\",\n",
    "    transforms=get_transforms(train=False),\n",
    "    image_size=HYPERPARAMETERS[\"image_size\"],\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=HYPERPARAMETERS[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ba6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select fixed validation images for consistent visualization\n",
    "val_indices = [0, 10, 25, 50]  # Fixed indices for tracking progress\n",
    "logger.set_validation_images(val_indices)\n",
    "\n",
    "# Pre-load these images\n",
    "fixed_val_images, fixed_val_targets = get_fixed_val_batch(val_dataset, val_indices, device)\n",
    "print(f\"Selected {len(val_indices)} validation images for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac29056",
   "metadata": {},
   "source": [
    "## 5. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = CustomMaskRCNN(\n",
    "    num_classes=HYPERPARAMETERS[\"num_classes\"],\n",
    "    pretrained_backbone=HYPERPARAMETERS[\"pretrained_backbone\"],\n",
    "    rpn_anchor_sizes=HYPERPARAMETERS[\"anchor_sizes\"],\n",
    "    rpn_aspect_ratios=HYPERPARAMETERS[\"aspect_ratios\"],\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Log model architecture to W&B\n",
    "wandb.watch(model, log=\"all\", log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e100397",
   "metadata": {},
   "source": [
    "## 6. Setup Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42497ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=HYPERPARAMETERS[\"learning_rate\"],\n",
    "    momentum=HYPERPARAMETERS[\"momentum\"],\n",
    "    weight_decay=HYPERPARAMETERS[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * HYPERPARAMETERS[\"num_epochs\"]\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=HYPERPARAMETERS[\"max_lr\"],\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.1,\n",
    ")\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a3ea8",
   "metadata": {},
   "source": [
    "## 7. Training Loop with W&B Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b69d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(HYPERPARAMETERS[\"num_epochs\"]):\n",
    "    logger.epoch = epoch\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TRAINING\n",
    "    # =========================================================================\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{HYPERPARAMETERS['num_epochs']}\")\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(pbar):\n",
    "        # Move to device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                   for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Skip empty batches\n",
    "        if all(len(t['boxes']) == 0 for t in targets):\n",
    "            continue\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        total_loss = sum(loss_dict.values())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # =====================================================================\n",
    "        # W&B LOGGING: Training metrics & Gradients (before optimizer step)\n",
    "        # =====================================================================\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Log training metrics (every N steps)\n",
    "        logger.log_training_step(\n",
    "            loss_dict=loss_dict,\n",
    "            learning_rate=current_lr,\n",
    "            step=global_step,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "        \n",
    "        # Log gradient norms for CBAM and RoI layers\n",
    "        logger.log_gradient_norms(model, step=global_step)\n",
    "        \n",
    "        # Optimizer step\n",
    "        torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "        global_step += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{total_loss.item():.4f}',\n",
    "            'lr': f'{current_lr:.6f}',\n",
    "        })\n",
    "    \n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # VALIDATION\n",
    "    # =========================================================================\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc=\"Validating\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                       for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            val_loss += sum(loss_dict.values()).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # W&B LOGGING: Validation metrics\n",
    "    # =========================================================================\n",
    "    logger.log_validation_metrics(\n",
    "        val_loss=avg_val_loss,\n",
    "        epoch=epoch,\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # W&B LOGGING: Validation predictions visualization\n",
    "    # =========================================================================\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get predictions on fixed validation images\n",
    "        val_images_device = [img.to(device) for img in fixed_val_images]\n",
    "        predictions = model(val_images_device)\n",
    "        \n",
    "        # Move predictions back to CPU for logging\n",
    "        predictions_cpu = [\n",
    "            {k: v.cpu() for k, v in pred.items()}\n",
    "            for pred in predictions\n",
    "        ]\n",
    "        \n",
    "        # Log visualization\n",
    "        logger.log_validation_predictions(\n",
    "            images=fixed_val_images,\n",
    "            targets=fixed_val_targets,\n",
    "            predictions=predictions_cpu,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # MODEL CHECKPOINTING\n",
    "    # =========================================================================\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = f\"checkpoints/epoch_{epoch+1}.pth\"\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_loss': avg_val_loss,\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Save best model and log as W&B artifact\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_path = \"checkpoints/best_model.pth\"\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        # Log best model as W&B artifact\n",
    "        logger.log_best_model(best_model_path, avg_val_loss)\n",
    "        print(f\"  -> New best model saved! Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8168e47",
   "metadata": {},
   "source": [
    "## 8. Finish W&B Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e23555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the W&B run\n",
    "logger.finish()\n",
    "\n",
    "print(f\"\\nW&B run completed!\")\n",
    "print(f\"View results at: {logger.run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef58701",
   "metadata": {},
   "source": [
    "## 9. Load Model from W&B Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe784e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load best model from W&B artifacts\n",
    "# Uncomment to use\n",
    "\n",
    "# api = wandb.Api()\n",
    "# artifact = api.artifact('YOUR_ENTITY/isaid-custom-segmentation/isaid-model:best')\n",
    "# artifact_dir = artifact.download()\n",
    "# \n",
    "# model = CustomMaskRCNN(num_classes=16)\n",
    "# model.load_state_dict(torch.load(f\"{artifact_dir}/best_model.pth\"))\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

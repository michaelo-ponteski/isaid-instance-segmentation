{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5aadee",
   "metadata": {},
   "source": [
    "# Full model pipeline for optimizing anchor sizes for RPN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8e7e0",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook provides a complete pipeline for optimizing RPN anchor sizes using:\n",
    "1. **Dataset Analysis** - Analyze object sizes and aspect ratios in your dataset\n",
    "2. **Data-Driven Suggestions** - Get initial anchor recommendations based on statistics\n",
    "3. **Optuna Optimization** - Fine-tune anchors using Bayesian hyperparameter search\n",
    "4. **Validation** - Compare default vs optimized anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e435aa",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32572d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Set memory optimization for CUDA\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c881b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install optuna if not available\n",
    "try:\n",
    "    import optuna\n",
    "    print(f\"Optuna version: {optuna.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing optuna...\")\n",
    "    !pip install optuna\n",
    "    import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6278511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from datasets.isaid_dataset import iSAIDDataset\n",
    "from training.transforms import get_transforms\n",
    "from training.anchor_optimizer import (\n",
    "    AnchorConfig,\n",
    "    AnchorOptimizer, \n",
    "    DatasetAnchorAnalyzer,\n",
    "    optimize_anchors_for_dataset,\n",
    "    analyze_dataset_anchors,\n",
    ")\n",
    "from models.maskrcnn_model import CustomMaskRCNN, get_custom_maskrcnn\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63506b4e",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1865bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Dataset\n",
    "    \"data_root\": \"../iSAID_patches\",  # Path to dataset\n",
    "    \"num_classes\": 16,                 # 15 classes + background\n",
    "    \"image_size\": 800,                 # Image size for training\n",
    "    \n",
    "    # Anchor Optimization\n",
    "    \"n_trials\": 20,                    # Number of Optuna trials (increase for better results)\n",
    "    \"num_analysis_samples\": 500,       # Samples for dataset analysis\n",
    "    \n",
    "    # Output\n",
    "    \"cache_path\": \"../optimized_anchors.pt\",  # Where to save optimized anchors\n",
    "}\n",
    "\n",
    "# Default anchor configuration (for comparison)\n",
    "DEFAULT_ANCHORS = {\n",
    "    \"sizes\": ((16, 24), (32, 48), (64, 96), (128, 192)),\n",
    "    \"aspect_ratios\": ((0.5, 1.0, 2.0),) * 4,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1e910",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ace62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets (without augmentation for analysis)\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "train_dataset = iSAIDDataset(\n",
    "    CONFIG[\"data_root\"],\n",
    "    split=\"train\",\n",
    "    transforms=get_transforms(train=False),\n",
    "    image_size=CONFIG[\"image_size\"],\n",
    ")\n",
    "\n",
    "val_dataset = iSAIDDataset(\n",
    "    CONFIG[\"data_root\"],\n",
    "    split=\"val\",\n",
    "    transforms=get_transforms(train=False),\n",
    "    image_size=CONFIG[\"image_size\"],\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f8494",
   "metadata": {},
   "source": [
    "## 4. Dataset Analysis\n",
    "\n",
    "First, let's analyze the object sizes and aspect ratios in the dataset to understand what anchor configurations might work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset for bounding box statistics\n",
    "analyzer = DatasetAnchorAnalyzer(train_dataset, num_samples=CONFIG[\"num_analysis_samples\"])\n",
    "stats = analyzer.compute_box_statistics()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Dataset Bounding Box Statistics\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Number of boxes analyzed: {len(stats['widths']):,}\")\n",
    "print(f\"\\nWidth:  min={stats['widths'].min():.1f}, max={stats['widths'].max():.1f}, mean={stats['widths'].mean():.1f}, std={stats['widths'].std():.1f}\")\n",
    "print(f\"Height: min={stats['heights'].min():.1f}, max={stats['heights'].max():.1f}, mean={stats['heights'].mean():.1f}, std={stats['heights'].std():.1f}\")\n",
    "print(f\"Area:   min={stats['areas'].min():.1f}, max={stats['areas'].max():.1f}, mean={stats['areas'].mean():.1f}\")\n",
    "print(f\"Aspect Ratio: min={stats['aspect_ratios'].min():.2f}, max={stats['aspect_ratios'].max():.2f}, mean={stats['aspect_ratios'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2599908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Object scales (sqrt of area)\n",
    "scales = np.sqrt(stats['areas'])\n",
    "axes[0, 0].hist(scales, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0, 0].axvline(np.median(scales), color='red', linestyle='--', label=f'Median: {np.median(scales):.1f}')\n",
    "axes[0, 0].set_xlabel('Object Scale (√area)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Object Scales')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Aspect ratios\n",
    "axes[0, 1].hist(stats['aspect_ratios'], bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[0, 1].axvline(np.median(stats['aspect_ratios']), color='red', linestyle='--', \n",
    "                   label=f'Median: {np.median(stats[\"aspect_ratios\"]):.2f}')\n",
    "axes[0, 1].set_xlabel('Aspect Ratio (w/h)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Aspect Ratios')\n",
    "axes[0, 1].set_xlim(0, 5)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Width vs Height scatter\n",
    "sample_idx = np.random.choice(len(stats['widths']), min(2000, len(stats['widths'])), replace=False)\n",
    "axes[1, 0].scatter(stats['widths'][sample_idx], stats['heights'][sample_idx], alpha=0.3, s=5)\n",
    "axes[1, 0].set_xlabel('Width')\n",
    "axes[1, 0].set_ylabel('Height')\n",
    "axes[1, 0].set_title('Width vs Height (sample)')\n",
    "axes[1, 0].set_aspect('equal')\n",
    "\n",
    "# Scale percentiles (for anchor size selection)\n",
    "percentiles = [10, 25, 50, 75, 90]\n",
    "scale_percentiles = np.percentile(scales, percentiles)\n",
    "axes[1, 1].barh(range(len(percentiles)), scale_percentiles, color='teal', edgecolor='black')\n",
    "axes[1, 1].set_yticks(range(len(percentiles)))\n",
    "axes[1, 1].set_yticklabels([f'{p}th' for p in percentiles])\n",
    "axes[1, 1].set_xlabel('Object Scale')\n",
    "axes[1, 1].set_title('Scale Percentiles')\n",
    "for i, v in enumerate(scale_percentiles):\n",
    "    axes[1, 1].text(v + 1, i, f'{v:.1f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScale percentiles (useful for anchor sizes):\")\n",
    "for p in [10, 20, 30, 50, 70, 80, 90]:\n",
    "    print(f\"  {p}th percentile: {np.percentile(scales, p):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01366bc7",
   "metadata": {},
   "source": [
    "## 5. Data-Driven Anchor Suggestions\n",
    "\n",
    "Get initial anchor recommendations based on dataset statistics (without Optuna optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data-driven suggestions\n",
    "suggested_sizes = analyzer.suggest_anchor_sizes(num_scales=4)\n",
    "suggested_ratios = analyzer.suggest_aspect_ratios(num_ratios=3)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Data-Driven Anchor Suggestions\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"\\nSuggested anchor sizes (per FPN level):\")\n",
    "for i, sizes in enumerate(suggested_sizes):\n",
    "    print(f\"  P{i+2}: {sizes}\")\n",
    "    \n",
    "print(f\"\\nSuggested aspect ratios: {suggested_ratios}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Comparison with Default Anchors\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"\\nDefault sizes:    {DEFAULT_ANCHORS['sizes']}\")\n",
    "print(f\"Suggested sizes:  {suggested_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04237dbe",
   "metadata": {},
   "source": [
    "## 6. Optuna Anchor Optimization\n",
    "\n",
    "Now run the full Optuna-based optimization to find the best anchor configuration by maximizing RPN recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the anchor optimizer\n",
    "optimizer = AnchorOptimizer(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    num_classes=CONFIG[\"num_classes\"],\n",
    "    device=device,\n",
    "    num_fpn_levels=4,\n",
    "    base_aspect_ratios=(0.5, 1.0, 2.0),\n",
    ")\n",
    "\n",
    "print(\"Optimizer initialized!\")\n",
    "print(f\"Data-suggested sizes: {optimizer.data_suggested_sizes}\")\n",
    "print(f\"Data-suggested ratios: {optimizer.data_suggested_ratios}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization (this may take a while)\n",
    "# Each trial creates a model, trains briefly, and evaluates RPN recall\n",
    "print(f\"Starting optimization with {CONFIG['n_trials']} trials...\")\n",
    "print(\"This may take 10-30 minutes depending on your hardware.\\n\")\n",
    "\n",
    "best_config = optimizer.optimize(\n",
    "    n_trials=CONFIG[\"n_trials\"],\n",
    "    study_name=\"anchor_optimization\",\n",
    "    storage=None,  # Use \"sqlite:///anchor_study.db\" for persistence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c61e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimized anchors\n",
    "torch.save({\n",
    "    'sizes': best_config.sizes,\n",
    "    'aspect_ratios': best_config.aspect_ratios,\n",
    "}, CONFIG[\"cache_path\"])\n",
    "\n",
    "print(f\"Optimized anchors saved to: {CONFIG['cache_path']}\")\n",
    "print(f\"\\nBest configuration found:\")\n",
    "print(f\"  Sizes: {best_config.sizes}\")\n",
    "print(f\"  Aspect ratios: {best_config.aspect_ratios}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce9651b",
   "metadata": {},
   "source": [
    "## 7. Validation: Compare Default vs Optimized Anchors\n",
    "\n",
    "Let's compare the RPN recall between the default and optimized anchor configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c5b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "\n",
    "def evaluate_anchor_config(anchor_sizes, anchor_ratios, name, num_batches=50):\n",
    "    \"\"\"Evaluate anchor configuration by computing RPN recall.\"\"\"\n",
    "    print(f\"\\nEvaluating: {name}\")\n",
    "    print(f\"  Sizes: {anchor_sizes}\")\n",
    "    print(f\"  Ratios: {anchor_ratios}\")\n",
    "    \n",
    "    # Create model with specified anchors\n",
    "    model = CustomMaskRCNN(\n",
    "        num_classes=CONFIG[\"num_classes\"],\n",
    "        rpn_anchor_sizes=anchor_sizes,\n",
    "        rpn_aspect_ratios=anchor_ratios,\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create validation loader\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "    \n",
    "    val_subset = Subset(val_dataset, range(min(100, len(val_dataset))))\n",
    "    val_loader = DataLoader(val_subset, batch_size=1, shuffle=False, \n",
    "                           collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    recalls_05 = []\n",
    "    recalls_075 = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(val_loader):\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                images = [img.to(device) for img in images]\n",
    "                images_tensor = torch.stack(images)\n",
    "                \n",
    "                original_sizes = [img.shape[-2:] for img in images]\n",
    "                image_list = ImageList(images_tensor, original_sizes)\n",
    "                \n",
    "                features = model.backbone(images_tensor)\n",
    "                proposals, _ = model.rpn(image_list, features, None)\n",
    "                \n",
    "                for props, target in zip(proposals, targets):\n",
    "                    gt_boxes = target['boxes'].to(device)\n",
    "                    if len(gt_boxes) == 0 or len(props) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    ious = box_iou(props, gt_boxes)\n",
    "                    max_ious, _ = ious.max(dim=0)\n",
    "                    \n",
    "                    recalls_05.append((max_ious >= 0.5).float().mean().item())\n",
    "                    recalls_075.append((max_ious >= 0.75).float().mean().item())\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    recall_05 = np.mean(recalls_05) if recalls_05 else 0.0\n",
    "    recall_075 = np.mean(recalls_075) if recalls_075 else 0.0\n",
    "    \n",
    "    print(f\"  Recall@0.5: {recall_05:.4f}\")\n",
    "    print(f\"  Recall@0.75: {recall_075:.4f}\")\n",
    "    \n",
    "    return {\"recall@0.5\": recall_05, \"recall@0.75\": recall_075}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ae76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate default anchors\n",
    "default_results = evaluate_anchor_config(\n",
    "    DEFAULT_ANCHORS[\"sizes\"],\n",
    "    DEFAULT_ANCHORS[\"aspect_ratios\"],\n",
    "    \"Default Anchors\"\n",
    ")\n",
    "\n",
    "# Evaluate optimized anchors\n",
    "optimized_results = evaluate_anchor_config(\n",
    "    best_config.sizes,\n",
    "    best_config.aspect_ratios,\n",
    "    \"Optimized Anchors\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82771c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics = ['recall@0.5', 'recall@0.75']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "default_vals = [default_results[m] for m in metrics]\n",
    "optimized_vals = [optimized_results[m] for m in metrics]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, default_vals, width, label='Default Anchors', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, optimized_vals, width, label='Optimized Anchors', color='coral')\n",
    "\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_title('RPN Recall Comparison: Default vs Optimized Anchors')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Recall@0.5', 'Recall@0.75'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print improvement\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Improvement Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "for m in metrics:\n",
    "    improvement = (optimized_results[m] - default_results[m]) / max(default_results[m], 1e-6) * 100\n",
    "    print(f\"{m}: {default_results[m]:.4f} → {optimized_results[m]:.4f} ({improvement:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d603ad",
   "metadata": {},
   "source": [
    "## 8. Usage: How to Use Optimized Anchors\n",
    "\n",
    "Copy the optimized configuration to use in your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574b17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the optimized configuration for copy-paste\n",
    "print(\"=\"*60)\n",
    "print(\"OPTIMIZED ANCHOR CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "# Option 1: Use directly in model creation\n",
    "model = CustomMaskRCNN(\n",
    "    num_classes={CONFIG['num_classes']},\n",
    "    rpn_anchor_sizes={best_config.sizes},\n",
    "    rpn_aspect_ratios={best_config.aspect_ratios},\n",
    ")\n",
    "\n",
    "# Option 2: Use with Trainer class\n",
    "trainer = Trainer(\n",
    "    data_root=\"{CONFIG['data_root']}\",\n",
    "    rpn_anchor_sizes={best_config.sizes},\n",
    "    rpn_aspect_ratios={best_config.aspect_ratios},\n",
    ")\n",
    "\n",
    "# Option 3: Load from cached file\n",
    "cached = torch.load(\"{CONFIG['cache_path']}\")\n",
    "model = CustomMaskRCNN(\n",
    "    num_classes={CONFIG['num_classes']},\n",
    "    rpn_anchor_sizes=cached['sizes'],\n",
    "    rpn_aspect_ratios=cached['aspect_ratios'],\n",
    ")\n",
    "\n",
    "# Option 4: Use automatic optimization in Trainer\n",
    "trainer = Trainer(\n",
    "    data_root=\"{CONFIG['data_root']}\",\n",
    "    optimize_anchors=True,  # Will use cached or re-optimize\n",
    "    anchor_cache_path=\"{CONFIG['cache_path']}\",\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332349c1",
   "metadata": {},
   "source": [
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd1427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del train_dataset, val_dataset, analyzer, optimizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

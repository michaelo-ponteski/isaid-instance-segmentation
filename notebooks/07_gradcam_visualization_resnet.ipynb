{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b3ac444",
   "metadata": {},
   "source": [
    "# Mask R-CNN Inference Visualization Pipeline (ResNet Backbone)\n",
    "\n",
    "This notebook provides comprehensive visualization of **every stage** of Mask R-CNN with **ResNet-50/101 backbone** during inference.\n",
    "\n",
    "## Pipeline Stages Visualized:\n",
    "1. **Input Image** - Original image preprocessing\n",
    "2. **ResNet Backbone** - Feature extraction at multiple scales (C2-C5)\n",
    "3. **Feature Pyramid Network (FPN)** - Multi-scale feature fusion (P2-P5)\n",
    "4. **Region Proposal Network (RPN)** - Anchor-based proposals\n",
    "5. **RoI Align** - Feature extraction with bilinear sampling\n",
    "6. **Box Head** - FC layers for classification and regression\n",
    "7. **Mask Head** - Convolutional layers for segmentation\n",
    "8. **Grad-CAM Heatmaps** - Class activation maps showing \"where the model looks\"\n",
    "9. **Final Predictions** - Boxes, classes, and masks\n",
    "\n",
    "**Note:** This notebook uses torchvision's pretrained ResNet-FPN Mask R-CNN instead of our custom EfficientNet + CBAM backbone.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7ad3c",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5167d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent if 'notebooks' in os.getcwd() else Path(os.getcwd())\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b210144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Enable interactive matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michaelo-ponteski/isaid-instance-segmentation.git\n",
    "%cd /kaggle/working/isaid-instance-segmentation\n",
    "!git pull\n",
    "!git switch gradcam\n",
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6bba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import visualization.gradcam_pipeline\n",
    "\n",
    "importlib.reload(visualization.gradcam_pipeline)\n",
    "\n",
    "from visualization.gradcam_pipeline import (\n",
    "    GradCAMConfig,\n",
    "    denormalize_image,\n",
    "    overlay_heatmap,\n",
    "    visualize_fpn_features,\n",
    "    visualize_roi_align_grid,\n",
    "    visualize_final_predictions,\n",
    "    ISAID_CLASS_LABELS,\n",
    "    ISAID_COLORS,\n",
    ")\n",
    "\n",
    "# Import torchvision's pretrained Mask R-CNN models\n",
    "from torchvision.models.detection import (\n",
    "    maskrcnn_resnet50_fpn,\n",
    "    maskrcnn_resnet50_fpn_v2,\n",
    "    MaskRCNN_ResNet50_FPN_Weights,\n",
    "    MaskRCNN_ResNet50_FPN_V2_Weights,\n",
    ")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "print(\"Visualization pipeline and ResNet models imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7751050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "my_secret = user_secrets.get_secret(\"wandb_key\")\n",
    "wandb.login(key=my_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35032a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init()\n",
    "# Update artifact path if you have a trained ResNet model\n",
    "artifact = run.use_artifact('marek-olnk-put-pozna-/isaid-custom-segmentation/isaid-maskrcnn-resnet50-final:v0', type='model') \n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd102e",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your model checkpoint path and image path here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c26764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION \n",
    "# =============================================================================\n",
    "\n",
    "# Backbone choice: \"resnet50\", \"resnet50_v2\", or \"resnet101\"\n",
    "BACKBONE_CHOICE = \"resnet50\"\n",
    "\n",
    "# Path to your trained model checkpoint\n",
    "CHECKPOINT_PATH = artifact_dir\n",
    "\n",
    "# Path to an image for inference (can be from validation set)\n",
    "IMAGE_PATH = \"/kaggle/input/isaid-patches/iSAID_patches/test/images/P0017_2400_3200_0_800.png\"\n",
    "# Buses and cars \"/kaggle/input/isaid-patches/iSAID_patches/test/images/P0015_0_800_600_1400.png\" \n",
    "# Airport \"/kaggle/input/isaid-patches/iSAID_patches/test/images/P0017_2400_3200_0_800.png\"\n",
    "\n",
    "# Or use a sample from the dataset\n",
    "USE_DATASET_SAMPLE = False  # Set to True to load from dataset\n",
    "DATASET_ROOT = \"data\"  # Path to iSAID dataset\n",
    "SAMPLE_INDEX = 0  # Which sample to visualize\n",
    "\n",
    "# Model configuration (must match training)\n",
    "NUM_CLASSES = 16  # iSAID has 15 classes + background\n",
    "\n",
    "# Visualization settings\n",
    "CONF_THRESHOLD = 0.5  # Confidence threshold for predictions\n",
    "OUTPUT_DIR = \"./gradcam_outputs_resnet\"  # Where to save visualizations\n",
    "\n",
    "print(\"Configuration set!\")\n",
    "print(f\"  Backbone: {BACKBONE_CHOICE}\")\n",
    "print(f\"  Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b109fb",
   "metadata": {},
   "source": [
    "## 3. Load Model and Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maskrcnn_resnet(num_classes, backbone_type=\"resnet50\", pretrained_coco=False):\n",
    "    \"\"\"\n",
    "    Create Mask R-CNN with pretrained ResNet backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of classes (including background)\n",
    "        backbone_type: \"resnet50\", \"resnet50_v2\", or \"resnet101\"\n",
    "        pretrained_coco: Whether to use COCO pretrained weights\n",
    "\n",
    "    Returns:\n",
    "        Mask R-CNN model\n",
    "    \"\"\"\n",
    "    if backbone_type == \"resnet50\":\n",
    "        if pretrained_coco:\n",
    "            weights = MaskRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "            model = maskrcnn_resnet50_fpn(weights=weights)\n",
    "        else:\n",
    "            model = maskrcnn_resnet50_fpn(\n",
    "                weights=None, weights_backbone=\"IMAGENET1K_V1\"\n",
    "            )\n",
    "\n",
    "    elif backbone_type == \"resnet50_v2\":\n",
    "        if pretrained_coco:\n",
    "            weights = MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "            model = maskrcnn_resnet50_fpn_v2(weights=weights)\n",
    "        else:\n",
    "            model = maskrcnn_resnet50_fpn_v2(\n",
    "                weights=None, weights_backbone=\"IMAGENET1K_V1\"\n",
    "            )\n",
    "\n",
    "    elif backbone_type == \"resnet101\":\n",
    "        from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "        from torchvision.models.detection import MaskRCNN\n",
    "        from torchvision.models import ResNet101_Weights\n",
    "\n",
    "        backbone = resnet_fpn_backbone(\n",
    "            backbone_name=\"resnet101\",\n",
    "            weights=ResNet101_Weights.IMAGENET1K_V1,\n",
    "            trainable_layers=5,\n",
    "        )\n",
    "        model = MaskRCNN(backbone, num_classes=num_classes)\n",
    "        return model\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backbone type: {backbone_type}\")\n",
    "\n",
    "    # Replace heads for our num_classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask, hidden_layer, num_classes\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create model\n",
    "print(f\"Creating Mask R-CNN with {BACKBONE_CHOICE} backbone...\")\n",
    "model = create_maskrcnn_resnet(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    backbone_type=BACKBONE_CHOICE,\n",
    "    pretrained_coco=False,  # We'll load trained weights\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  - Backbone: {BACKBONE_CHOICE} with FPN\")\n",
    "print(f\"  - Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "print(f\"  - Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76322974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = PROJECT_ROOT / CHECKPOINT_PATH / \"best_model.pth\"\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint = torch.load(str(checkpoint_path), map_location=device)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at: {checkpoint_path}\")\n",
    "    print(\"Using randomly initialized weights for demonstration.\")\n",
    "    print(\"\\nTo use trained weights, update CHECKPOINT_PATH in the configuration cell.\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model moved to {device} and set to eval mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87346f11",
   "metadata": {},
   "source": [
    "## 4. Load Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be82849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "if USE_DATASET_SAMPLE:\n",
    "    try:\n",
    "        from datasets.isaid_dataset import get_isaid_dataset\n",
    "        from training.transforms import get_transform\n",
    "        \n",
    "        val_dataset = get_isaid_dataset(\n",
    "            root=str(PROJECT_ROOT / DATASET_ROOT),\n",
    "            split='val',\n",
    "            transforms=get_transform(train=False),\n",
    "        )\n",
    "        \n",
    "        image_tensor, target = val_dataset[SAMPLE_INDEX]\n",
    "        image_np = denormalize_image(image_tensor)\n",
    "        \n",
    "        print(f\"Loaded sample {SAMPLE_INDEX} from validation set\")\n",
    "        print(f\"  Image shape: {image_tensor.shape}\")\n",
    "        print(f\"  Number of GT objects: {len(target['boxes'])}\")\n",
    "        \n",
    "        gt_classes = [ISAID_CLASS_LABELS[l.item()] for l in target['labels']]\n",
    "        print(f\"  GT classes: {gt_classes}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load from dataset: {e}\")\n",
    "        print(\"Falling back to image path...\")\n",
    "        USE_DATASET_SAMPLE = False\n",
    "\n",
    "if not USE_DATASET_SAMPLE:\n",
    "    image_path = Path(IMAGE_PATH)\n",
    "    \n",
    "    if image_path.exists():\n",
    "        image_np = np.array(Image.open(image_path).convert('RGB'))\n",
    "        print(f\"Loaded image from {image_path}\")\n",
    "        print(f\"  Image shape: {image_np.shape}\")\n",
    "    else:\n",
    "        print(f\"Image not found at {image_path}\")\n",
    "        print(\"Creating a dummy test image...\")\n",
    "        image_np = np.random.randint(0, 255, (800, 800, 3), dtype=np.uint8)\n",
    "\n",
    "# Display the input image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_np)\n",
    "plt.title('Input Image', fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c393687",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Stage-by-Stage Visualization\n",
    "\n",
    "Now we'll visualize each stage of the inference pipeline step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4223d13",
   "metadata": {},
   "source": [
    "## Stage 1: Preprocessing & Feature Extraction Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Preprocess image for ResNet (standard ImageNet normalization)\n",
    "def preprocess_image(image_np):\n",
    "    \"\"\"Preprocess image for inference.\"\"\"\n",
    "    # Convert to tensor and normalize\n",
    "    image_tensor = F.to_tensor(image_np)\n",
    "    # ImageNet normalization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    image_tensor = (image_tensor - mean) / std\n",
    "    return image_tensor\n",
    "\n",
    "# Store extracted features using hooks\n",
    "extracted_features = {}\n",
    "fpn_features = {}\n",
    "hooks = []\n",
    "\n",
    "def make_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, dict):\n",
    "            for k, v in output.items():\n",
    "                extracted_features[f\"{name}_{k}\"] = v.detach()\n",
    "        else:\n",
    "            extracted_features[name] = output.detach() if isinstance(output, torch.Tensor) else output\n",
    "    return hook\n",
    "\n",
    "# Register hooks for ResNet backbone\n",
    "# Backbone layers\n",
    "hooks.append(model.backbone.body.layer1.register_forward_hook(make_hook('resnet_layer1')))\n",
    "hooks.append(model.backbone.body.layer2.register_forward_hook(make_hook('resnet_layer2')))\n",
    "hooks.append(model.backbone.body.layer3.register_forward_hook(make_hook('resnet_layer3')))\n",
    "hooks.append(model.backbone.body.layer4.register_forward_hook(make_hook('resnet_layer4')))\n",
    "\n",
    "# FPN layers\n",
    "def fpn_hook(module, input, output):\n",
    "    if isinstance(output, dict):\n",
    "        for k, v in output.items():\n",
    "            fpn_features[k] = v.detach()\n",
    "hooks.append(model.backbone.fpn.register_forward_hook(fpn_hook))\n",
    "\n",
    "# Box head\n",
    "hooks.append(model.roi_heads.box_head.register_forward_hook(make_hook('box_head')))\n",
    "\n",
    "print(\"Registered forward hooks for feature extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2103da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "print(\"Running inference with feature extraction hooks...\")\n",
    "\n",
    "image_tensor = preprocess_image(image_np).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model([image_tensor])[0]\n",
    "\n",
    "# Move predictions to CPU\n",
    "predictions = {k: v.cpu() if isinstance(v, torch.Tensor) else v for k, v in predictions.items()}\n",
    "\n",
    "print(f\"\\nâœ“ Inference complete!\")\n",
    "print(f\"   Detections found: {len(predictions['boxes'])}\")\n",
    "print(f\"   Extracted features: {len(extracted_features)} tensors\")\n",
    "print(f\"   FPN levels: {list(fpn_features.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a20088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all extracted features\n",
    "print(\"\\nðŸ“Š Extracted Features Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, tensor in sorted(extracted_features.items()):\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        shape = list(tensor.shape)\n",
    "        print(f\"  {name:40s} -> {shape}\")\n",
    "\n",
    "print(\"\\nðŸ“Š FPN Features:\")\n",
    "for name, tensor in sorted(fpn_features.items()):\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        shape = list(tensor.shape)\n",
    "        print(f\"  {name:40s} -> {shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ffc9d",
   "metadata": {},
   "source": [
    "## Stage 2: ResNet Backbone Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ResNet backbone stages\n",
    "backbone_stages = {\n",
    "    'resnet_layer1': 'Layer 1 (C2)',\n",
    "    'resnet_layer2': 'Layer 2 (C3)', \n",
    "    'resnet_layer3': 'Layer 3 (C4)',\n",
    "    'resnet_layer4': 'Layer 4 (C5)',\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "ax_idx = 0\n",
    "for stage_key, stage_name in backbone_stages.items():\n",
    "    if stage_key in extracted_features:\n",
    "        feat = extracted_features[stage_key]\n",
    "        if feat.dim() == 4:\n",
    "            feat = feat[0]\n",
    "        \n",
    "        # Mean activation\n",
    "        feat_mean = feat.mean(dim=0).cpu().numpy()\n",
    "        feat_mean = (feat_mean - feat_mean.min()) / (feat_mean.max() - feat_mean.min() + 1e-8)\n",
    "        \n",
    "        axes[ax_idx].imshow(feat_mean, cmap='viridis')\n",
    "        axes[ax_idx].set_title(f'{stage_name}\\nMean ({feat.shape[0]} channels)', fontsize=10)\n",
    "        axes[ax_idx].axis('off')\n",
    "        ax_idx += 1\n",
    "        \n",
    "        # Max activation\n",
    "        feat_max = feat.max(dim=0)[0].cpu().numpy()\n",
    "        feat_max = (feat_max - feat_max.min()) / (feat_max.max() - feat_max.min() + 1e-8)\n",
    "        \n",
    "        axes[ax_idx].imshow(feat_max, cmap='hot')\n",
    "        axes[ax_idx].set_title(f'{stage_name}\\nMax Activation', fontsize=10)\n",
    "        axes[ax_idx].axis('off')\n",
    "        ax_idx += 1\n",
    "\n",
    "# Hide unused axes\n",
    "for i in range(ax_idx, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "fig.suptitle('Stage 2: ResNet Backbone Feature Maps', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06e223",
   "metadata": {},
   "source": [
    "## Stage 3: Feature Pyramid Network (FPN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FPN multi-scale features\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "fpn_levels = ['0', '1', '2', '3']  # ResNet FPN uses numeric keys\n",
    "level_names = ['P2', 'P3', 'P4', 'P5']\n",
    "\n",
    "for idx, (level, name) in enumerate(zip(fpn_levels, level_names)):\n",
    "    if level in fpn_features:\n",
    "        feat = fpn_features[level]\n",
    "        if feat.dim() == 4:\n",
    "            feat = feat[0]\n",
    "        \n",
    "        # Mean activation\n",
    "        feat_mean = feat.mean(dim=0).cpu().numpy()\n",
    "        feat_mean = (feat_mean - feat_mean.min()) / (feat_mean.max() - feat_mean.min() + 1e-8)\n",
    "        \n",
    "        # Top row: Raw feature map\n",
    "        axes[0, idx].imshow(feat_mean, cmap='plasma')\n",
    "        axes[0, idx].set_title(f'{name}: {feat.shape[1]}x{feat.shape[2]}\\n({feat.shape[0]} channels)', fontsize=10)\n",
    "        axes[0, idx].axis('off')\n",
    "        \n",
    "        # Bottom row: Overlay on image\n",
    "        feat_resized = cv2.resize(feat_mean, (image_np.shape[1], image_np.shape[0]))\n",
    "        overlay = overlay_heatmap(image_np, feat_resized, alpha=0.5, colormap='plasma')\n",
    "        \n",
    "        axes[1, idx].imshow(overlay)\n",
    "        axes[1, idx].set_title(f'{name} Overlay on Image', fontsize=10)\n",
    "        axes[1, idx].axis('off')\n",
    "\n",
    "fig.suptitle('Stage 3: FPN Feature Maps at Each Scale', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c70f23",
   "metadata": {},
   "source": [
    "## Stage 4: RPN Proposals (Region Proposal Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf2946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detected boxes as RPN-like proposals\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "boxes = predictions['boxes'].cpu().numpy()\n",
    "scores = predictions['scores'].cpu().numpy()\n",
    "labels = predictions['labels'].cpu().numpy()\n",
    "\n",
    "# Left: All predictions colored by confidence\n",
    "axes[0].imshow(image_np)\n",
    "cmap = plt.get_cmap('RdYlGn')\n",
    "\n",
    "for box, score in zip(boxes, scores):\n",
    "    x1, y1, x2, y2 = box\n",
    "    color = cmap(score)\n",
    "    rect = patches.Rectangle(\n",
    "        (x1, y1), x2-x1, y2-y1,\n",
    "        linewidth=1, edgecolor=color, facecolor='none', alpha=0.7\n",
    "    )\n",
    "    axes[0].add_patch(rect)\n",
    "\n",
    "axes[0].set_title(f'All Predictions ({len(boxes)} boxes)\\nColored by confidence (red=low, green=high)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Right: High confidence predictions only\n",
    "axes[1].imshow(image_np)\n",
    "\n",
    "high_conf = scores >= CONF_THRESHOLD\n",
    "for box, score, label in zip(boxes[high_conf], scores[high_conf], labels[high_conf]):\n",
    "    x1, y1, x2, y2 = box\n",
    "    color = np.array(ISAID_COLORS.get(label, [255, 255, 255])) / 255.0\n",
    "    \n",
    "    rect = patches.Rectangle(\n",
    "        (x1, y1), x2-x1, y2-y1,\n",
    "        linewidth=2, edgecolor=color, facecolor='none'\n",
    "    )\n",
    "    axes[1].add_patch(rect)\n",
    "    axes[1].text(x1, y1-3, f'{ISAID_CLASS_LABELS[label]}: {score:.2f}',\n",
    "                 fontsize=8, color='white', backgroundcolor=color)\n",
    "\n",
    "axes[1].set_title(f'High Confidence (>{CONF_THRESHOLD}) Predictions ({high_conf.sum()} boxes)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "fig.suptitle('Stage 4: Region Proposals and NMS Results', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd98f3",
   "metadata": {},
   "source": [
    "## Stage 5: RoI Align Sampling Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RoI Align sampling grids\n",
    "if len(predictions['boxes']) > 0:\n",
    "    top_k = min(4, len(predictions['boxes']))\n",
    "    top_boxes = predictions['boxes'][:top_k]\n",
    "    \n",
    "    fig = visualize_roi_align_grid(\n",
    "        image_np,\n",
    "        top_boxes,\n",
    "        output_size=7,\n",
    "        sampling_ratio=2,\n",
    "        max_boxes=top_k,\n",
    "        figsize=(20, 6)\n",
    "    )\n",
    "    fig.suptitle('Stage 5: RoI Align - Bilinear Sampling Grid Visualization', fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No detections to visualize RoI Align grid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee142678",
   "metadata": {},
   "source": [
    "## Stage 6: Box Head Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53574725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize box head features\n",
    "if 'box_head' in extracted_features and len(predictions['boxes']) > 0:\n",
    "    box_feats = extracted_features['box_head']\n",
    "    \n",
    "    # Box head outputs flattened features per RoI\n",
    "    n_rois = min(8, box_feats.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_rois):\n",
    "        feat = box_feats[i].cpu().numpy()\n",
    "        \n",
    "        # Reshape to 2D for visualization if needed\n",
    "        if feat.ndim == 1:\n",
    "            side = int(np.sqrt(len(feat)))\n",
    "            if side * side == len(feat):\n",
    "                feat_2d = feat.reshape(side, side)\n",
    "            else:\n",
    "                feat_2d = feat[:256].reshape(16, 16) if len(feat) >= 256 else feat.reshape(-1, 1)\n",
    "        else:\n",
    "            feat_2d = feat\n",
    "        \n",
    "        axes[i].imshow(feat_2d, cmap='viridis')\n",
    "        if i < len(predictions['labels']):\n",
    "            label = predictions['labels'][i].item()\n",
    "            score = predictions['scores'][i].item()\n",
    "            axes[i].set_title(f'RoI {i+1}: {ISAID_CLASS_LABELS[label]}\\nConf: {score:.2f}', fontsize=10)\n",
    "        else:\n",
    "            axes[i].set_title(f'RoI {i+1}', fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    for i in range(n_rois, 8):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    fig.suptitle('Stage 6: Box Head Features per RoI', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Box head features not captured or no detections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c8da5",
   "metadata": {},
   "source": [
    "## Stage 7: Mask Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461470c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize individual mask predictions\n",
    "if 'masks' in predictions and len(predictions['masks']) > 0:\n",
    "    masks = predictions['masks'].cpu().numpy()\n",
    "    \n",
    "    n_masks = min(8, len(masks))\n",
    "    fig, axes = plt.subplots(2, n_masks, figsize=(4*n_masks, 8))\n",
    "    if n_masks == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for i in range(n_masks):\n",
    "        mask = masks[i]\n",
    "        if mask.ndim == 3:\n",
    "            mask = mask[0]  # Remove channel dim\n",
    "        \n",
    "        label = predictions['labels'][i].item()\n",
    "        score = predictions['scores'][i].item()\n",
    "        box = predictions['boxes'][i].cpu().numpy()\n",
    "        \n",
    "        # Top: Raw mask probability\n",
    "        axes[0, i].imshow(mask, cmap='hot', vmin=0, vmax=1)\n",
    "        axes[0, i].set_title(f'{ISAID_CLASS_LABELS[label]}\\nP={score:.2f}', fontsize=10)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Bottom: Binary mask on image crop\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        crop = image_np[max(0,y1):y2, max(0,x1):x2].copy()\n",
    "        mask_binary = (mask > 0.5).astype(np.uint8)\n",
    "        mask_crop = mask_binary[max(0,y1):y2, max(0,x1):x2]\n",
    "        \n",
    "        if crop.size > 0 and mask_crop.size > 0:\n",
    "            if mask_crop.shape != crop.shape[:2]:\n",
    "                mask_crop = cv2.resize(mask_crop, (crop.shape[1], crop.shape[0]))\n",
    "            \n",
    "            color = np.array(ISAID_COLORS.get(label, [255, 0, 0]))\n",
    "            overlay = crop.copy().astype(np.float32)\n",
    "            overlay[mask_crop > 0] = overlay[mask_crop > 0] * 0.5 + color * 0.5\n",
    "            overlay = np.clip(overlay, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            axes[1, i].imshow(overlay)\n",
    "        axes[1, i].set_title('Masked Region', fontsize=10)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    fig.suptitle('Stage 7: Individual Mask Predictions', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No mask predictions available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2566939a",
   "metadata": {},
   "source": [
    "## Stage 8: Grad-CAM Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Grad-CAM style heatmaps from different FPN levels\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "levels = ['0', '1', '2', '3']\n",
    "level_names = ['P2', 'P3', 'P4', 'P5']\n",
    "\n",
    "for idx, (level, name) in enumerate(zip(levels, level_names)):\n",
    "    if level in fpn_features:\n",
    "        feat = fpn_features[level]\n",
    "        if feat.dim() == 4:\n",
    "            feat = feat[0]\n",
    "        \n",
    "        # Compute activation-based heatmap (Grad-CAM approximation)\n",
    "        heatmap = feat.mean(dim=0).cpu().numpy()\n",
    "        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "        heatmap = cv2.resize(heatmap, (image_np.shape[1], image_np.shape[0]))\n",
    "        \n",
    "        # Top row: Heatmap only\n",
    "        axes[0, idx].imshow(heatmap, cmap='jet')\n",
    "        axes[0, idx].set_title(f'{name} Activation Map', fontsize=12)\n",
    "        axes[0, idx].axis('off')\n",
    "        \n",
    "        # Bottom row: Overlay on image\n",
    "        overlay = overlay_heatmap(image_np, heatmap, alpha=0.5, colormap='jet')\n",
    "        axes[1, idx].imshow(overlay)\n",
    "        axes[1, idx].set_title(f'{name} Overlay', fontsize=12)\n",
    "        axes[1, idx].axis('off')\n",
    "\n",
    "fig.suptitle('Stage 8: Grad-CAM Heatmaps - Where the Model Looks at Each FPN Level', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142840cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined multi-scale heatmap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(image_np)\n",
    "axes[0].set_title('Original Image', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Combine heatmaps from multiple scales\n",
    "combined_heatmap = np.zeros((image_np.shape[0], image_np.shape[1]), dtype=np.float32)\n",
    "weights = {'0': 0.1, '1': 0.2, '2': 0.3, '3': 0.4}  # Higher weight for deeper features\n",
    "\n",
    "for level, weight in weights.items():\n",
    "    if level in fpn_features:\n",
    "        feat = fpn_features[level]\n",
    "        if feat.dim() == 4:\n",
    "            feat = feat[0]\n",
    "        \n",
    "        heatmap = feat.mean(dim=0).cpu().numpy()\n",
    "        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "        heatmap = cv2.resize(heatmap, (image_np.shape[1], image_np.shape[0]))\n",
    "        combined_heatmap += weight * heatmap\n",
    "\n",
    "# Normalize combined\n",
    "combined_heatmap = (combined_heatmap - combined_heatmap.min()) / (combined_heatmap.max() - combined_heatmap.min() + 1e-8)\n",
    "\n",
    "# Multi-scale heatmap\n",
    "axes[1].imshow(combined_heatmap, cmap='jet')\n",
    "axes[1].set_title('Multi-Scale Combined Heatmap', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Overlay\n",
    "overlay = overlay_heatmap(image_np, combined_heatmap, alpha=0.5, colormap='jet')\n",
    "axes[2].imshow(overlay)\n",
    "axes[2].set_title('Combined Heatmap Overlay', fontsize=14)\n",
    "axes[2].axis('off')\n",
    "\n",
    "fig.suptitle('Multi-Scale Grad-CAM: Combined Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801007ad",
   "metadata": {},
   "source": [
    "## Stage 9: Final Predictions with Full Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive visualization\n",
    "fig = visualize_final_predictions(\n",
    "    image_np,\n",
    "    predictions,\n",
    "    gradcam_heatmap=combined_heatmap,\n",
    "    class_labels=ISAID_CLASS_LABELS,\n",
    "    conf_threshold=CONF_THRESHOLD,\n",
    "    figsize=(18, 6)\n",
    ")\n",
    "fig.suptitle('Stage 9: Final Predictions with Masks and Grad-CAM', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detection summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "boxes = predictions['boxes'].cpu().numpy()\n",
    "labels = predictions['labels'].cpu().numpy()\n",
    "scores = predictions['scores'].cpu().numpy()\n",
    "\n",
    "high_conf = scores >= CONF_THRESHOLD\n",
    "print(f\"\\nTotal detections: {len(boxes)}\")\n",
    "print(f\"High confidence (>{CONF_THRESHOLD}): {high_conf.sum()}\")\n",
    "\n",
    "print(f\"\\n{'#':<4} {'Class':<20} {'Confidence':<12} {'Box (x1,y1,x2,y2)'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, (box, label, score) in enumerate(zip(boxes[high_conf], labels[high_conf], scores[high_conf])):\n",
    "    class_name = ISAID_CLASS_LABELS.get(label, f'Class {label}')\n",
    "    box_str = f\"({box[0]:.0f}, {box[1]:.0f}, {box[2]:.0f}, {box[3]:.0f})\"\n",
    "    print(f\"{i+1:<4} {class_name:<20} {score:<12.4f} {box_str}\")\n",
    "\n",
    "# Class distribution\n",
    "if high_conf.sum() > 0:\n",
    "    print(\"\\nðŸ“Š Class Distribution:\")\n",
    "    unique_labels, counts = np.unique(labels[high_conf], return_counts=True)\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        class_name = ISAID_CLASS_LABELS.get(label, f'Class {label}')\n",
    "        print(f\"   {class_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "print(\"Hooks removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0980c3de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook visualized the complete inference pipeline of Mask R-CNN with **ResNet backbone**:\n",
    "\n",
    "| Stage | Component | What We Saw |\n",
    "|-------|-----------|-------------|\n",
    "| 1 | Input | Original image preprocessing |\n",
    "| 2 | Backbone | ResNet feature extraction (C2-C5) |\n",
    "| 3 | FPN | Multi-scale feature fusion (P2-P5) |\n",
    "| 4 | RPN | Region proposals generation |\n",
    "| 5 | RoI Align | Bilinear sampling grid for each proposal |\n",
    "| 6 | Box Head | Classification and bbox regression |\n",
    "| 7 | Mask Head | Instance segmentation prediction |\n",
    "| 8 | Grad-CAM | Where the model \"looks\" to make predictions |\n",
    "| 9 | Output | Final boxes, classes, masks, and confidence scores |\n",
    "\n",
    "**Key Differences from Custom EfficientNet + CBAM:**\n",
    "- No CBAM attention modules (standard ResNet blocks)\n",
    "- Deeper backbone with more parameters\n",
    "- Standard FPN without additional attention\n",
    "- Pretrained on COCO (transfer learning)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
